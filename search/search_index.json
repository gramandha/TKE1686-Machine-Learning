{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Selamat Datang di Gramandha-docs Materi Perkuliahan Machine Learning Kode MK TKE1686 - 3 SKS Dokumen ini dibuat untuk pembelajaran machine learning di Teknik Elektro, Universitas Jember by Gramandha Wega Intyanto, S.ST., M.T. Dokumen : Silabus, RPS, Kontrak Kuliah CPL CPL 4. Mampu merencanakan manajemen berbasis prinsip keteknikan, keamanan, keselamatan kerja, kesehatan, dan membuat keputusan yang tepat CPL 5. Mampu menganalisis konsep matematika, sains, dan prinsip rekayasa (engineering principles) untuk menyelesaikan masalah rekayasa kompleks pada sistem tenaga listrik, sistem elektronika dan kendali, atau telekomunikasi CPL 7. Mampu merancang sistem tenaga listrik, sistem elektronika dan kendali, atau telekomunikasi dengan pendekatan analitis dan mempertimbangkan standar teknis, aspek kinerja, keandalan, kemudahan penerapan, dan keberlanjutan CPMK Mampu mempresentasikan hasil perencanaan manajemen keteknikan Mampu menerapkan prinsip matematis, sains, dan keteknikan untuk problem solving secara numerik maupun analisis Mampu menganalisis data dan relasi antar parameter Sub CPMK Mampu memahami konsep Artificial Intelegence/Machine Learning Mampu memahami konsep data Mampu merancang model - Clustering (Unsupervised) Mampu merancang model - Klasifikasi (Supervised) Mampu merancang model - Regresi (Supervised) Mampu merancang model - Reinforcement Learning Mampu mengevaluasi model Mampu mengaplikasikan model yang dibuat Mampu menentukan, merumuskan, merancang, mengevaluasi dan menyimpulkan untuk project based machine learning Daftar Isi BAB 1: Pengantar Machine Learning pada bidang Teknik Elektro BAB 2: Dataset BAB 3: Klaster BAB 4: Klasifikasi Referensi Tom M. Mitchell, Machine Learning, McGraw-Hill, 1997 Alberto Artasanchez dan Prateek Joshi, Artificial Intelligence with Python, Packt Publishing, 2020 Aurelien Geron, Hands-On Machine Learning with Scikit-Learn, Keras, and Tensorflow: Concepts, Tool, and Techniques, O\u2019Reilly Media, 2017 Eklas Hossain, Machine Learning Course for Engineers, Springer, 2024","title":"Home"},{"location":"#selamat-datang-di-gramandha-docs","text":"","title":"Selamat Datang di Gramandha-docs"},{"location":"#materi-perkuliahan-machine-learning","text":"","title":"Materi Perkuliahan Machine Learning"},{"location":"#kode-mk-tke1686-3-sks","text":"Dokumen ini dibuat untuk pembelajaran machine learning di Teknik Elektro, Universitas Jember by Gramandha Wega Intyanto, S.ST., M.T.","title":"Kode MK TKE1686 - 3 SKS"},{"location":"#dokumen-silabus-rps-kontrak-kuliah","text":"","title":"Dokumen : Silabus, RPS, Kontrak Kuliah"},{"location":"#cpl","text":"CPL 4. Mampu merencanakan manajemen berbasis prinsip keteknikan, keamanan, keselamatan kerja, kesehatan, dan membuat keputusan yang tepat CPL 5. Mampu menganalisis konsep matematika, sains, dan prinsip rekayasa (engineering principles) untuk menyelesaikan masalah rekayasa kompleks pada sistem tenaga listrik, sistem elektronika dan kendali, atau telekomunikasi CPL 7. Mampu merancang sistem tenaga listrik, sistem elektronika dan kendali, atau telekomunikasi dengan pendekatan analitis dan mempertimbangkan standar teknis, aspek kinerja, keandalan, kemudahan penerapan, dan keberlanjutan","title":"CPL"},{"location":"#cpmk","text":"Mampu mempresentasikan hasil perencanaan manajemen keteknikan Mampu menerapkan prinsip matematis, sains, dan keteknikan untuk problem solving secara numerik maupun analisis Mampu menganalisis data dan relasi antar parameter","title":"CPMK"},{"location":"#sub-cpmk","text":"Mampu memahami konsep Artificial Intelegence/Machine Learning Mampu memahami konsep data Mampu merancang model - Clustering (Unsupervised) Mampu merancang model - Klasifikasi (Supervised) Mampu merancang model - Regresi (Supervised) Mampu merancang model - Reinforcement Learning Mampu mengevaluasi model Mampu mengaplikasikan model yang dibuat Mampu menentukan, merumuskan, merancang, mengevaluasi dan menyimpulkan untuk project based machine learning","title":"Sub CPMK"},{"location":"#daftar-isi","text":"BAB 1: Pengantar Machine Learning pada bidang Teknik Elektro BAB 2: Dataset BAB 3: Klaster BAB 4: Klasifikasi","title":"Daftar Isi"},{"location":"#referensi","text":"Tom M. Mitchell, Machine Learning, McGraw-Hill, 1997 Alberto Artasanchez dan Prateek Joshi, Artificial Intelligence with Python, Packt Publishing, 2020 Aurelien Geron, Hands-On Machine Learning with Scikit-Learn, Keras, and Tensorflow: Concepts, Tool, and Techniques, O\u2019Reilly Media, 2017 Eklas Hossain, Machine Learning Course for Engineers, Springer, 2024","title":"Referensi"},{"location":"LKM1/","text":"LKM 1 Dosen Pengampu : Gramandha Wega Intyanto, S.ST., M.T Jenis Tugas: Individu Pengumpulan mmp Pertemuan 2 (LKM 1) Detail Tugas Silahkan cari studi kasus pada bidang Anda yang dapat diselesaikan dengan ML. Beri 2 contoh tiap teknik( supervised learning dan unspervised learning ) satu masalah! Cari dataset terkait studi kasus tersebut kemudian jelaskan proses penyelesaian dengan algoritma apa! Note: Rekomendasi tempat untuk mencari dataset: kaggle , uci , google dataset","title":"LKM 1"},{"location":"LKM1/#lkm-1","text":"Dosen Pengampu : Gramandha Wega Intyanto, S.ST., M.T Jenis Tugas: Individu Pengumpulan mmp Pertemuan 2 (LKM 1)","title":"LKM 1"},{"location":"LKM1/#detail-tugas","text":"Silahkan cari studi kasus pada bidang Anda yang dapat diselesaikan dengan ML. Beri 2 contoh tiap teknik( supervised learning dan unspervised learning ) satu masalah! Cari dataset terkait studi kasus tersebut kemudian jelaskan proses penyelesaian dengan algoritma apa! Note: Rekomendasi tempat untuk mencari dataset: kaggle , uci , google dataset","title":"Detail Tugas"},{"location":"LKM2/","text":"LKM 2 Dosen Pengampu : Gramandha Wega Intyanto, S.ST., M.T Jenis Tugas: Individu Pengumpulan mmp Pertemuan 3 (LKM 2) Detail Tugas Silahkan lakukan pra-pemrosesan Data pada dataset Anda yang sudah dicari, jika belum ada gunakan dataset yang sudah disediakan!","title":"LKM 2"},{"location":"LKM2/#lkm-2","text":"Dosen Pengampu : Gramandha Wega Intyanto, S.ST., M.T Jenis Tugas: Individu Pengumpulan mmp Pertemuan 3 (LKM 2)","title":"LKM 2"},{"location":"LKM2/#detail-tugas","text":"Silahkan lakukan pra-pemrosesan Data pada dataset Anda yang sudah dicari, jika belum ada gunakan dataset yang sudah disediakan!","title":"Detail Tugas"},{"location":"LKM3/","text":"LKM 3 Dosen Pengampu : Gramandha Wega Intyanto, S.ST., M.T Jenis Tugas: Kelompok Pengumpulan mmp Pertemuan 4 (LKM 3) Detail Tugas Cari dataset yang perlu untuk di klaster atau dari dataset yang Anda punya, silahkan lakukan klaster dari featur yang Anda tentukan, kemudian klaster dengan K-mean Klaster, Hirarki Klaster, dan DBscan.","title":"LKM 3"},{"location":"LKM3/#lkm-3","text":"Dosen Pengampu : Gramandha Wega Intyanto, S.ST., M.T Jenis Tugas: Kelompok Pengumpulan mmp Pertemuan 4 (LKM 3)","title":"LKM 3"},{"location":"LKM3/#detail-tugas","text":"Cari dataset yang perlu untuk di klaster atau dari dataset yang Anda punya, silahkan lakukan klaster dari featur yang Anda tentukan, kemudian klaster dengan K-mean Klaster, Hirarki Klaster, dan DBscan.","title":"Detail Tugas"},{"location":"LKM4/","text":"LKM 4 1. Soal Klasterisasi dengan K-means dan Hierarchical Clustering Deskripsi Soal Sebuah perusahaan ingin mengelompokkan pelanggan berdasarkan pola pembelian mereka. Data yang tersedia mencakup informasi penghasilan bulanan (dalam juta rupiah) dan jumlah transaksi dalam sebulan . Gunakan K-means dan Hierarchical Clustering untuk mengelompokkan pelanggan dan tentukan pola yang terbentuk. Data Pelanggan ID Pelanggan Penghasilan (Juta Rupiah) Jumlah Transaksi per Bulan P1 3.5 5 P2 7.0 20 P3 2.0 3 P4 5.5 10 P5 8.0 25 P6 1.5 2 P7 6.0 12 P8 9.5 30 P9 4.0 7 P10 3.0 4 Tugas a. K-means Clustering Jika kita sudah memiliki klaster awal dengan pusat klaster sebagai berikut: Klaster 1 : (2.5, 4) Klaster 2 : (7.5, 22) Gunakan metode K-means untuk mengelompokkan pelanggan ke dalam salah satu dari klaster tersebut berdasarkan jarak Euclidean. Tentukan klaster akhir setelah semua pelanggan dikategorikan. b. Hierarchical Clustering Gunakan metode Agglomerative Hierarchical Clustering dengan pendekatan Single Linkage atau Complete Linkage . Buat dendrogram untuk menunjukkan proses penggabungan klaster hingga terbentuk satu klaster besar. Tentukan jumlah klaster optimal berdasarkan dendrogram. 2. Soal Klasifikasi dengan k-NN dan Decision Tree Deskripsi Soal Sebuah bank ingin mengklasifikasikan calon nasabah ke dalam kategori \"Layak Kredit\" atau \"Tidak Layak Kredit\" berdasarkan beberapa parameter. Data yang tersedia mencakup penghasilan bulanan , jumlah pinjaman yang sedang berjalan , dan status pekerjaan . Gunakan k-NN dan Decision Tree untuk melakukan klasifikasi dan tentukan pola yang terbentuk. Data Nasabah ID Nasabah Penghasilan (Juta Rupiah) Pinjaman Aktif (Juta) Status Pekerjaan Status Kredit (Label) N1 5.0 10 Tetap Layak Kredit N2 2.5 5 Kontrak Tidak Layak Kredit N3 7.0 20 Tetap Layak Kredit N4 3.0 8 Kontrak Tidak Layak Kredit N5 6.0 15 Tetap Layak Kredit N6 4.0 12 Kontrak Tidak Layak Kredit N7 8.5 25 Tetap Layak Kredit N8 3.5 6 Kontrak Tidak Layak Kredit N9 6.5 18 Tetap Layak Kredit N10 2.0 4 Kontrak Tidak Layak Kredit Tugas a. K-NN Classification Gunakan metode k-Nearest Neighbors (k-NN) untuk mengklasifikasikan nasabah baru dengan fitur berikut: Penghasilan = 4.5 juta rupiah Pinjaman Aktif = 9 juta rupiah Status Pekerjaan = Kontrak Gunakan k = 3 dan hitung jarak Euclidean untuk menentukan kelas nasabah tersebut. b. Decision Tree Classification Gunakan metode Decision Tree untuk membangun model klasifikasi berdasarkan fitur yang tersedia. Gambarkan struktur decision tree yang terbentuk. Gunakan model untuk memprediksi status kredit nasabah dengan parameter berikut: Penghasilan = 6.0 juta rupiah Pinjaman Aktif = 14 juta rupiah Status Pekerjaan = Tetap","title":"LKM 4"},{"location":"LKM4/#lkm-4","text":"","title":"LKM 4"},{"location":"LKM4/#1-soal-klasterisasi-dengan-k-means-dan-hierarchical-clustering","text":"","title":"1. Soal Klasterisasi dengan K-means dan Hierarchical Clustering"},{"location":"LKM4/#deskripsi-soal","text":"Sebuah perusahaan ingin mengelompokkan pelanggan berdasarkan pola pembelian mereka. Data yang tersedia mencakup informasi penghasilan bulanan (dalam juta rupiah) dan jumlah transaksi dalam sebulan . Gunakan K-means dan Hierarchical Clustering untuk mengelompokkan pelanggan dan tentukan pola yang terbentuk.","title":"Deskripsi Soal"},{"location":"LKM4/#data-pelanggan","text":"ID Pelanggan Penghasilan (Juta Rupiah) Jumlah Transaksi per Bulan P1 3.5 5 P2 7.0 20 P3 2.0 3 P4 5.5 10 P5 8.0 25 P6 1.5 2 P7 6.0 12 P8 9.5 30 P9 4.0 7 P10 3.0 4","title":"Data Pelanggan"},{"location":"LKM4/#tugas","text":"","title":"Tugas"},{"location":"LKM4/#a-k-means-clustering","text":"Jika kita sudah memiliki klaster awal dengan pusat klaster sebagai berikut: Klaster 1 : (2.5, 4) Klaster 2 : (7.5, 22) Gunakan metode K-means untuk mengelompokkan pelanggan ke dalam salah satu dari klaster tersebut berdasarkan jarak Euclidean. Tentukan klaster akhir setelah semua pelanggan dikategorikan.","title":"a. K-means Clustering"},{"location":"LKM4/#b-hierarchical-clustering","text":"Gunakan metode Agglomerative Hierarchical Clustering dengan pendekatan Single Linkage atau Complete Linkage . Buat dendrogram untuk menunjukkan proses penggabungan klaster hingga terbentuk satu klaster besar. Tentukan jumlah klaster optimal berdasarkan dendrogram.","title":"b. Hierarchical Clustering"},{"location":"LKM4/#2-soal-klasifikasi-dengan-k-nn-dan-decision-tree","text":"","title":"2. Soal Klasifikasi dengan k-NN dan Decision Tree"},{"location":"LKM4/#deskripsi-soal_1","text":"Sebuah bank ingin mengklasifikasikan calon nasabah ke dalam kategori \"Layak Kredit\" atau \"Tidak Layak Kredit\" berdasarkan beberapa parameter. Data yang tersedia mencakup penghasilan bulanan , jumlah pinjaman yang sedang berjalan , dan status pekerjaan . Gunakan k-NN dan Decision Tree untuk melakukan klasifikasi dan tentukan pola yang terbentuk.","title":"Deskripsi Soal"},{"location":"LKM4/#data-nasabah","text":"ID Nasabah Penghasilan (Juta Rupiah) Pinjaman Aktif (Juta) Status Pekerjaan Status Kredit (Label) N1 5.0 10 Tetap Layak Kredit N2 2.5 5 Kontrak Tidak Layak Kredit N3 7.0 20 Tetap Layak Kredit N4 3.0 8 Kontrak Tidak Layak Kredit N5 6.0 15 Tetap Layak Kredit N6 4.0 12 Kontrak Tidak Layak Kredit N7 8.5 25 Tetap Layak Kredit N8 3.5 6 Kontrak Tidak Layak Kredit N9 6.5 18 Tetap Layak Kredit N10 2.0 4 Kontrak Tidak Layak Kredit","title":"Data Nasabah"},{"location":"LKM4/#tugas_1","text":"","title":"Tugas"},{"location":"LKM4/#a-k-nn-classification","text":"Gunakan metode k-Nearest Neighbors (k-NN) untuk mengklasifikasikan nasabah baru dengan fitur berikut: Penghasilan = 4.5 juta rupiah Pinjaman Aktif = 9 juta rupiah Status Pekerjaan = Kontrak Gunakan k = 3 dan hitung jarak Euclidean untuk menentukan kelas nasabah tersebut.","title":"a. K-NN Classification"},{"location":"LKM4/#b-decision-tree-classification","text":"Gunakan metode Decision Tree untuk membangun model klasifikasi berdasarkan fitur yang tersedia. Gambarkan struktur decision tree yang terbentuk. Gunakan model untuk memprediksi status kredit nasabah dengan parameter berikut: Penghasilan = 6.0 juta rupiah Pinjaman Aktif = 14 juta rupiah Status Pekerjaan = Tetap","title":"b. Decision Tree Classification"},{"location":"Materi1/","text":"Pertemuan 2: Pengantar Machine Learning pada bidang Teknik Elektro [TKE1686] MK Machine Learning (3-SKS) oleh Gramandha Wega Intyanto 1. Definisi Machine Learning Machine Learning (ML) adalah cabang dari kecerdasan buatan (Artificial Intelligence, AI) yang memungkinkan sistem untuk belajar dari data tanpa diprogram secara eksplisit. ML menggunakan algoritma dan model statistik untuk mengenali pola dalam data dan membuat keputusan atau prediksi berdasarkan pola tersebut. Definisi dari beberapa ahli Arthur Samuel (1959) : \" Machine Learning adalah bidang studi terkait pemberian kemampuan pada komputer untuk belajar tanpa diprogram secara eksplisit.\" Tom Mitchell (1997) : \" Machine Learning adalah suatu program komputer yang belajar dari pengalaman (experience E) terhadap tugas (Task T) dengan ukuran performa (performance P) , jika performanya terhadap tugas (Task T) meningkat dengan pengalaman (experience E) .\" Dapat disimpulkan bahwa machine learning merupakan bidang ilmu yang mempelajari dari data dan mengeksekusi hasil dari pembelajaran data tersebut. 2. Sejarah Singkat Machine Learning 1950-an: Alan Turing memperkenalkan Turing Test untuk mengukur kecerdasan mesin. Frank Rosenblatt mengembangkan perceptron, model awal dari neural network. 1970-an s.d. 1980-an: Pengembangan algoritma decision tree dan neural network pertama. Backpropagation mulai digunakan dalam pelatihan neural network. 1990-an: Munculnya Support Vector Machine (SVM) dan Random Forest sebagai metode klasifikasi canggih. Mulai berkembangnya aplikasi ML dalam pattern recognition dan data mining. 2000-an s.d. Sekarang: Kemajuan dalam Deep Learning dan Neural Networks berkat peningkatan daya komputasi dan ketersediaan big data. Model seperti Convolutional Neural Network (CNN) dan Recurrent Neural Network (RNN) menjadi dasar dalam pengolahan citra dan sinyal. Aplikasi ML berkembang pesat di berbagai bidang, termasuk kesehatan, finansial, transportasi, dan teknik elektro. 3. Klasifikasi Machine Learning yang akan di pelajari Ditarik dari permasalahan pembelajaran (learning problem) dalam konteks ML pada dataset (sejumlah n sample data) untuk melakukan prediksi terhadap properties yang tidak diketahui pada dataset lain yang sejenis. Secara umum untuk penyelesain permasalahan pembelajaran, Machine Learning dibagi menjadi dua teknik dasar yang akan kita pelajari pada mata kuliah (2), yaitu supervised learning dan unsupervised learning Supervised Learning Teknik atau model pembelajaran yang diterapkan pada mesin dari pola dalam data yang memiliki label tertentu contoh kasus: 1. Deteksi kesalah paa jaringan listrik, dimana data yang dimiliki yaitu tegangan, arus, frekuensi dalam jaringan. Kemudian tiap data memiliki label jaringan berstatus: Normal dan Gangguan 2. Robot akan melakukan klasifikasi jenis bunga, dimana data yang memiliki kelopak bunga, bentuk kelopak, warna, dsb. Kemudian tiap data memiliki label jenis bunga: Mawar dan Bunga Matahari Pada supervised learning ini pada umumnya memiliki minimal 2 data, data input yaitu pola pada data dan data output yaitu kategori (label). source : geeksforgeeks Tipe dari Supervised Learning yaitu: Klasifikasi: Di mana output adalah variabel kategoris (mis., Email spam vs non-spam, ya vs tidak). Regresi: Di mana output adalah variabel kontinu (mis., Memprediksi harga rumah, harga saham). Contoh algoritma atau model yang sering digunakan pada bidang elektro yaitu Regresi, Suppert Vector Machine (SVM), K-Nearest Neighbors (K-NN), Random Forest biasanya digunakan untuk klasifikasi, dsb. Berikut contoh algoritma yang lebih lengkap terkait Supervised Learning Algoritma Regresi, Klasifikasi Tujuan Metode Kasus Penggunaan Regresi Linear Regresi Memprediksi nilai output kontinu Persamaan linear yang meminimalkan jumlah kuadrat residu Memprediksi nilai kontinu Regresi Logistik Klasifikasi Memprediksi variabel output biner Fungsi logistik yang mentransformasikan hubungan linear Tugas klasifikasi biner Pohon Keputusan Keduanya Memodelkan keputusan dan hasil Struktur pohon dengan keputusan dan hasil Tugas klasifikasi dan regresi Random Forest Keduanya Meningkatkan akurasi klasifikasi dan regresi Menggabungkan beberapa pohon keputusan Mengurangi overfitting, meningkatkan akurasi prediksi SVM Keduanya Membuat hyperplane untuk klasifikasi atau prediksi nilai kontinu Memaksimalkan margin antara kelas atau memprediksi nilai kontinu Tugas klasifikasi dan regresi KNN Keduanya Memprediksi kelas atau nilai berdasarkan k tetangga terdekat Mencari k tetangga terdekat dan memprediksi berdasarkan mayoritas atau rata-rata Tugas klasifikasi dan regresi, sensitif terhadap data berisik Gradient Boosting Keduanya Menggabungkan model lemah untuk membuat model yang kuat Mengoreksi kesalahan secara iteratif dengan model baru Tugas klasifikasi dan regresi untuk meningkatkan akurasi prediksi Naive Bayes Klasifikasi Memprediksi kelas berdasarkan asumsi independensi fitur Teorema Bayes dengan asumsi independensi fitur Klasifikasi teks, penyaringan spam, analisis sentimen, medis Unsupervised Learning Teknik atau model pembelajaran yang diterapkan pada mesin dari pola dalam data yang tidak memiliki label tertentu Contoh kasus Operator jaringan lisrik ingin mengkelompokkan pelanggan berdasarkan pola komsumsi energi untuk meningkatan efiseiensi ditribusi daya. Pada unsupervised learning ini pada umumnya memiliki minimal 1 data, data input yaitu pola pada data. source : geeksforgeeks Tipe dari Unsupervised Learning yaitu: Clustering Algorithms : Pengelompokan dalam pembelajaran mesin tanpa pengawasan adalah proses pengelompokan data yang tidak berlabel ke dalam kelompok berdasarkan kesamaan mereka. Tujuan pengelompokan adalah untuk mengidentifikasi pola dan hubungan dalam data tanpa pengetahuan sebelumnya tentang makna data. Association Rule Learning : Pembelajaran aturan asosiasi juga dikenal sebagai penambangan aturan asosiasi adalah teknik umum yang digunakan untuk menemukan asosiasi dalam pembelajaran mesin tanpa pengawasan. Dimensionality Reduction : Pengurangan dimensi adalah proses mengurangi jumlah fitur dalam dataset sambil menjaga informasi sebanyak mungkin. Teknik ini berguna untuk meningkatkan kinerja algoritma pembelajaran mesin dan untuk visualisasi data. Contoh algoritma atau model yang digunakan pada yaitu K-means cluster Contoh dataset ML Supervised Unsupervised 3. Peran Machine Learning dalam Teknik Elektro Machine Learning memiliki peran penting dalam berbagai aspek Teknik Elektro, termasuk pemrosesan sinyal, pengendalian sistem, dan optimasi kinerja perangkat elektronik. Beberapa penerapan utama ML dalam Teknik Elektro adalah: a. Pemrosesan Sinyal dan Citra Digital Deteksi dan klasifikasi sinyal menggunakan Fourier Transform dan Wavelet Transform. Pengenalan pola dalam sinyal kelistrikan, seperti fault detection dalam jaringan listrik. Penerapan ML dalam pengolahan citra digital untuk mendeteksi objek atau menganalisis data dari kamera industri. b. Sistem Kendali dan Otomasi Machine Learning digunakan dalam sistem kontrol adaptif, misalnya pada robotik dan otomasi industri. Peningkatan efisiensi sistem kendali berbasis fuzzy logic dan neural network. c. Prediksi dan Optimasi dalam Jaringan Listrik Memprediksi konsumsi daya listrik menggunakan time-series forecasting. Optimasi distribusi daya menggunakan ML untuk meningkatkan efisiensi grid listrik. d. Machine Learning dalam Embedded Systems Implementasi TinyML (Machine Learning pada perangkat dengan daya rendah seperti mikroprosesor dan FPGA). Aplikasi ML dalam Internet of Things (IoT) untuk sistem pemantauan cerdas. e. Keamanan dan Deteksi Anomali Pendeteksian anomali dalam jaringan listrik untuk mencegah blackout atau gangguan listrik. Sistem keamanan berbasis ML dalam deteksi peretasan jaringan komunikasi dan kendali. 4. Tools yang digunakan untuk ML Pada akhir tahun ini perancangan ML bahasa program yang sering digunakan yaitu Python dengan beberapa tools library berikut: Framework dan Library ML - TensorFlow : Framework open-source untuk deep learning dan ML yang dikembangkan oleh Google. - PyTorch : Framework ML berbasis Python dengan fleksibilitas tinggi, dikembangkan oleh Facebook AI. - Scikit-learn : Library ML berbasis Python untuk klasifikasi, regresi, dan clustering. - Keras : API tinggi berbasis TensorFlow untuk pengembangan model deep learning. Tools untuk Pemrosesan Data dan Visualisasi - Pandas : Library Python untuk manipulasi dan analisis data berbasis tabel. - NumPy : Library untuk operasi numerik dan array multidimensi dalam Python. - Matplotlib : Library visualisasi untuk membuat grafik dan plot data. - Seaborn : Library berbasis Matplotlib untuk visualisasi data yang lebih menarik. - Dask : Library untuk pemrosesan data besar yang dapat berjalan secara paralel. *Note: Kalau Anda ingin mencoba-coba ada software yang sudah include untuk penerapan ML yaitu WEKA 4. Tugas Mahasiswa Silahkan cari studi kasus pada bidang Anda yang dapat diselesaikan dengan ML. Beri 2 contoh tiap teknik( supervised learning dan unspervised learning ) satu masalah! Cari dataset terkait studi kasus tersebut kemudian jelaskan proses penyelesaian dengan algoritma apa! Note: Rekomendasi tempat untuk mencari dataset: kaggle , uci , google dataset","title":"Pertemuan 2: Pengantar Machine Learning pada bidang Teknik Elektro"},{"location":"Materi1/#pertemuan-2-pengantar-machine-learning-pada-bidang-teknik-elektro","text":"[TKE1686] MK Machine Learning (3-SKS) oleh Gramandha Wega Intyanto","title":"Pertemuan 2: Pengantar Machine Learning pada bidang Teknik Elektro"},{"location":"Materi1/#1-definisi-machine-learning","text":"Machine Learning (ML) adalah cabang dari kecerdasan buatan (Artificial Intelligence, AI) yang memungkinkan sistem untuk belajar dari data tanpa diprogram secara eksplisit. ML menggunakan algoritma dan model statistik untuk mengenali pola dalam data dan membuat keputusan atau prediksi berdasarkan pola tersebut.","title":"1. Definisi Machine Learning"},{"location":"Materi1/#definisi-dari-beberapa-ahli","text":"Arthur Samuel (1959) : \" Machine Learning adalah bidang studi terkait pemberian kemampuan pada komputer untuk belajar tanpa diprogram secara eksplisit.\" Tom Mitchell (1997) : \" Machine Learning adalah suatu program komputer yang belajar dari pengalaman (experience E) terhadap tugas (Task T) dengan ukuran performa (performance P) , jika performanya terhadap tugas (Task T) meningkat dengan pengalaman (experience E) .\" Dapat disimpulkan bahwa machine learning merupakan bidang ilmu yang mempelajari dari data dan mengeksekusi hasil dari pembelajaran data tersebut.","title":"Definisi dari beberapa ahli"},{"location":"Materi1/#2-sejarah-singkat-machine-learning","text":"","title":"2. Sejarah Singkat Machine Learning"},{"location":"Materi1/#1950-an","text":"Alan Turing memperkenalkan Turing Test untuk mengukur kecerdasan mesin. Frank Rosenblatt mengembangkan perceptron, model awal dari neural network.","title":"1950-an:"},{"location":"Materi1/#1970-an-sd-1980-an","text":"Pengembangan algoritma decision tree dan neural network pertama. Backpropagation mulai digunakan dalam pelatihan neural network.","title":"1970-an s.d. 1980-an:"},{"location":"Materi1/#1990-an","text":"Munculnya Support Vector Machine (SVM) dan Random Forest sebagai metode klasifikasi canggih. Mulai berkembangnya aplikasi ML dalam pattern recognition dan data mining.","title":"1990-an:"},{"location":"Materi1/#2000-an-sd-sekarang","text":"Kemajuan dalam Deep Learning dan Neural Networks berkat peningkatan daya komputasi dan ketersediaan big data. Model seperti Convolutional Neural Network (CNN) dan Recurrent Neural Network (RNN) menjadi dasar dalam pengolahan citra dan sinyal. Aplikasi ML berkembang pesat di berbagai bidang, termasuk kesehatan, finansial, transportasi, dan teknik elektro.","title":"2000-an s.d. Sekarang:"},{"location":"Materi1/#3-klasifikasi-machine-learning-yang-akan-di-pelajari","text":"Ditarik dari permasalahan pembelajaran (learning problem) dalam konteks ML pada dataset (sejumlah n sample data) untuk melakukan prediksi terhadap properties yang tidak diketahui pada dataset lain yang sejenis. Secara umum untuk penyelesain permasalahan pembelajaran, Machine Learning dibagi menjadi dua teknik dasar yang akan kita pelajari pada mata kuliah (2), yaitu supervised learning dan unsupervised learning","title":"3. Klasifikasi Machine Learning yang akan di pelajari"},{"location":"Materi1/#supervised-learning","text":"Teknik atau model pembelajaran yang diterapkan pada mesin dari pola dalam data yang memiliki label tertentu contoh kasus: 1. Deteksi kesalah paa jaringan listrik, dimana data yang dimiliki yaitu tegangan, arus, frekuensi dalam jaringan. Kemudian tiap data memiliki label jaringan berstatus: Normal dan Gangguan 2. Robot akan melakukan klasifikasi jenis bunga, dimana data yang memiliki kelopak bunga, bentuk kelopak, warna, dsb. Kemudian tiap data memiliki label jenis bunga: Mawar dan Bunga Matahari Pada supervised learning ini pada umumnya memiliki minimal 2 data, data input yaitu pola pada data dan data output yaitu kategori (label). source : geeksforgeeks Tipe dari Supervised Learning yaitu: Klasifikasi: Di mana output adalah variabel kategoris (mis., Email spam vs non-spam, ya vs tidak). Regresi: Di mana output adalah variabel kontinu (mis., Memprediksi harga rumah, harga saham). Contoh algoritma atau model yang sering digunakan pada bidang elektro yaitu Regresi, Suppert Vector Machine (SVM), K-Nearest Neighbors (K-NN), Random Forest biasanya digunakan untuk klasifikasi, dsb. Berikut contoh algoritma yang lebih lengkap terkait Supervised Learning Algoritma Regresi, Klasifikasi Tujuan Metode Kasus Penggunaan Regresi Linear Regresi Memprediksi nilai output kontinu Persamaan linear yang meminimalkan jumlah kuadrat residu Memprediksi nilai kontinu Regresi Logistik Klasifikasi Memprediksi variabel output biner Fungsi logistik yang mentransformasikan hubungan linear Tugas klasifikasi biner Pohon Keputusan Keduanya Memodelkan keputusan dan hasil Struktur pohon dengan keputusan dan hasil Tugas klasifikasi dan regresi Random Forest Keduanya Meningkatkan akurasi klasifikasi dan regresi Menggabungkan beberapa pohon keputusan Mengurangi overfitting, meningkatkan akurasi prediksi SVM Keduanya Membuat hyperplane untuk klasifikasi atau prediksi nilai kontinu Memaksimalkan margin antara kelas atau memprediksi nilai kontinu Tugas klasifikasi dan regresi KNN Keduanya Memprediksi kelas atau nilai berdasarkan k tetangga terdekat Mencari k tetangga terdekat dan memprediksi berdasarkan mayoritas atau rata-rata Tugas klasifikasi dan regresi, sensitif terhadap data berisik Gradient Boosting Keduanya Menggabungkan model lemah untuk membuat model yang kuat Mengoreksi kesalahan secara iteratif dengan model baru Tugas klasifikasi dan regresi untuk meningkatkan akurasi prediksi Naive Bayes Klasifikasi Memprediksi kelas berdasarkan asumsi independensi fitur Teorema Bayes dengan asumsi independensi fitur Klasifikasi teks, penyaringan spam, analisis sentimen, medis","title":"Supervised Learning"},{"location":"Materi1/#unsupervised-learning","text":"Teknik atau model pembelajaran yang diterapkan pada mesin dari pola dalam data yang tidak memiliki label tertentu Contoh kasus Operator jaringan lisrik ingin mengkelompokkan pelanggan berdasarkan pola komsumsi energi untuk meningkatan efiseiensi ditribusi daya. Pada unsupervised learning ini pada umumnya memiliki minimal 1 data, data input yaitu pola pada data. source : geeksforgeeks Tipe dari Unsupervised Learning yaitu: Clustering Algorithms : Pengelompokan dalam pembelajaran mesin tanpa pengawasan adalah proses pengelompokan data yang tidak berlabel ke dalam kelompok berdasarkan kesamaan mereka. Tujuan pengelompokan adalah untuk mengidentifikasi pola dan hubungan dalam data tanpa pengetahuan sebelumnya tentang makna data. Association Rule Learning : Pembelajaran aturan asosiasi juga dikenal sebagai penambangan aturan asosiasi adalah teknik umum yang digunakan untuk menemukan asosiasi dalam pembelajaran mesin tanpa pengawasan. Dimensionality Reduction : Pengurangan dimensi adalah proses mengurangi jumlah fitur dalam dataset sambil menjaga informasi sebanyak mungkin. Teknik ini berguna untuk meningkatkan kinerja algoritma pembelajaran mesin dan untuk visualisasi data. Contoh algoritma atau model yang digunakan pada yaitu K-means cluster","title":"Unsupervised Learning"},{"location":"Materi1/#contoh-dataset-ml","text":"Supervised Unsupervised","title":"Contoh dataset ML"},{"location":"Materi1/#3-peran-machine-learning-dalam-teknik-elektro","text":"Machine Learning memiliki peran penting dalam berbagai aspek Teknik Elektro, termasuk pemrosesan sinyal, pengendalian sistem, dan optimasi kinerja perangkat elektronik. Beberapa penerapan utama ML dalam Teknik Elektro adalah:","title":"3. Peran Machine Learning dalam Teknik Elektro"},{"location":"Materi1/#a-pemrosesan-sinyal-dan-citra-digital","text":"Deteksi dan klasifikasi sinyal menggunakan Fourier Transform dan Wavelet Transform. Pengenalan pola dalam sinyal kelistrikan, seperti fault detection dalam jaringan listrik. Penerapan ML dalam pengolahan citra digital untuk mendeteksi objek atau menganalisis data dari kamera industri.","title":"a. Pemrosesan Sinyal dan Citra Digital"},{"location":"Materi1/#b-sistem-kendali-dan-otomasi","text":"Machine Learning digunakan dalam sistem kontrol adaptif, misalnya pada robotik dan otomasi industri. Peningkatan efisiensi sistem kendali berbasis fuzzy logic dan neural network.","title":"b. Sistem Kendali dan Otomasi"},{"location":"Materi1/#c-prediksi-dan-optimasi-dalam-jaringan-listrik","text":"Memprediksi konsumsi daya listrik menggunakan time-series forecasting. Optimasi distribusi daya menggunakan ML untuk meningkatkan efisiensi grid listrik.","title":"c. Prediksi dan Optimasi dalam Jaringan Listrik"},{"location":"Materi1/#d-machine-learning-dalam-embedded-systems","text":"Implementasi TinyML (Machine Learning pada perangkat dengan daya rendah seperti mikroprosesor dan FPGA). Aplikasi ML dalam Internet of Things (IoT) untuk sistem pemantauan cerdas.","title":"d. Machine Learning dalam Embedded Systems"},{"location":"Materi1/#e-keamanan-dan-deteksi-anomali","text":"Pendeteksian anomali dalam jaringan listrik untuk mencegah blackout atau gangguan listrik. Sistem keamanan berbasis ML dalam deteksi peretasan jaringan komunikasi dan kendali.","title":"e. Keamanan dan Deteksi Anomali"},{"location":"Materi1/#4-tools-yang-digunakan-untuk-ml","text":"Pada akhir tahun ini perancangan ML bahasa program yang sering digunakan yaitu Python dengan beberapa tools library berikut: Framework dan Library ML - TensorFlow : Framework open-source untuk deep learning dan ML yang dikembangkan oleh Google. - PyTorch : Framework ML berbasis Python dengan fleksibilitas tinggi, dikembangkan oleh Facebook AI. - Scikit-learn : Library ML berbasis Python untuk klasifikasi, regresi, dan clustering. - Keras : API tinggi berbasis TensorFlow untuk pengembangan model deep learning. Tools untuk Pemrosesan Data dan Visualisasi - Pandas : Library Python untuk manipulasi dan analisis data berbasis tabel. - NumPy : Library untuk operasi numerik dan array multidimensi dalam Python. - Matplotlib : Library visualisasi untuk membuat grafik dan plot data. - Seaborn : Library berbasis Matplotlib untuk visualisasi data yang lebih menarik. - Dask : Library untuk pemrosesan data besar yang dapat berjalan secara paralel. *Note: Kalau Anda ingin mencoba-coba ada software yang sudah include untuk penerapan ML yaitu WEKA","title":"4. Tools yang digunakan untuk ML"},{"location":"Materi1/#4-tugas-mahasiswa","text":"Silahkan cari studi kasus pada bidang Anda yang dapat diselesaikan dengan ML. Beri 2 contoh tiap teknik( supervised learning dan unspervised learning ) satu masalah! Cari dataset terkait studi kasus tersebut kemudian jelaskan proses penyelesaian dengan algoritma apa! Note: Rekomendasi tempat untuk mencari dataset: kaggle , uci , google dataset","title":"4. Tugas Mahasiswa"},{"location":"Materi2/","text":"Pertemuan 3: Pengumpulan, Pemrosesan dan Visualisasi pada Dataset [TKE1686] MK Machine Learning (3-SKS) oleh Gramandha Wega Intyanto 1. Alur Kerja pada Machine Learning graph LR; A[\"Tahap 1\\nPengumpulan Dataset\"] --> B[\"Tahap 2\\nPemrosesan Dataset\"]; B --> C[\"Tahap 3\\nPembelajaran Model\"]; C --> D[\"Tahap 4\\nEvaluasi Model\"]; D -.->|Perbaikan pemrosesan Dataset| B; D -.->|Revisi Model| C; D -.->|Perbaikan Dataset| A; Pengumpulan Dataset: Langkah awal ML adalah mengumpulkan kumpulan data. Hal-hal yang harus dipertimbangakan dalam pengumpulan data yaitu metode pengambilan data, data akan digunakan untuk apa? Preproses Data: Data yang kami kumpulkan sering kali tidak terorganisir dan tidak dapat langsung digunakan untuk melatih model. Sebelum melanjutkan ke langkah berikutnya, data perlu diproses terlebih dahulu. Kumpulan data mungkin berisi data yang hilang atau berisik, perlu di perbaiki atau di filter ; Data yang berbeda dapat berada dalam rentang yang berbeda, perlu dilakukan standarisasi atau normalisasi karena bisa jadi menjadi masalah bagi model; Perlu menemukan dan memilih data yang lebih berkontribusi untuk menemukan variabel target; Terakhir, kumpulan data harus dibagi menjadi dua set yaitu pelatihan dan pengujian (bisa juga dilakukan pembagian menjadi 3 yaitu pelatihan, validasi, pengujian ) | yang sering digunakan 80% set pelatihan, 20% set pengujian / 70% set pelatihan 20% set validasi 10% set pengujian. Melatih Model: Berdasarkan permasalahan, jenis model yang dibutuhkan harus dipilih terlebih dahulu. Saat memilih model, informasi yang tersedia pada kumpulan data harus dipertimbangkan. Evaluasi Model: Setelah model dibangun dan dilatih, penting untuk memahami seberapa baik model telah dilatih, seberapa baik kinerjanya, dan apakah model akan berguna untuk eksperimen. Kumpulan data set pengujiandigunakan untuk menguji model , dan berbagai metrik evaluasi digunakan untuk mengevaluasi dan memahami model. 2. Dataset Performa model sangat bergantung pada persiapan kumpulan data (dataset). Biasanya Kumpulan data paling baik disusun dalam bentuk tabel, di mana setiap baris sesuai dengan satu entri data dan setiap kolom mewakili variabel tertentu yang terkait dengan semua data yang tersedia dalam kumpulan data tersebut. Ukuran kumpulan data yang berkisar dari terabyte hingga megabyte dapat disebut sebagai kumpulan data besar, sedangkan kumpulan data dalam kisaran kilobyte adalah kumpulan data kecil. 2.1. Pengumpulan Data a. Data yang diambil langsung dari secara langsung dengan beberapa komponen sensor-sensor. - Kamera dan sensor optik untuk pengambilan data citra. - Senosr IMU untuk pengambilan data percepatan, rotasi dan orientasi. - Bisa juga dataset kumpulan beberapa sensor seperti data loging - dsb. b. Data yang diambil langsung dari beberapa platform data public - (roboflow, keaggle, coco, pascal voc, dsb) Beberapa Rekomendasi Dataset untuk Machine Learning dalam Teknik Elektro 1. Dataset untuk Pengolahan Citra (Computer Vision) - MNIST Dataset angka tulisan tangan (0-9) untuk klasifikasi. - CIFAR-10 & CIFAR-100 Dataset gambar untuk klasifikasi objek. - COCO (Common Objects in Context) Dataset objek dunia nyata untuk deteksi dan segmentasi. - Open Electrical Substation Images Dataset gambar infrastruktur listrik. 2. Dataset untuk Pemrosesan Sinyal dan Sistem Tenaga - EEG Brainwave Dataset Data sinyal EEG untuk analisis otak. - ECG Heartbeat Categorization Dataset Sinyal jantung ECG untuk klasifikasi kondisi kesehatan. - Electricity Load Forecasting Dataset konsumsi energi listrik untuk prediksi. - Fault Detection in Power Systems Dataset deteksi gangguan pada jaringan listrik. 3. Dataset untuk Kendali dan Robotika - Self-Driving Car Dataset (Udacity) Dataset untuk pengendalian kendaraan otonom. - MIT Push Dataset Dataset untuk robotik dan manipulasi objek. - CMU Motion Capture Dataset Data pergerakan tubuh manusia untuk kontrol robot. 4. Dataset untuk Prediksi dan Analisis Data - Weather Dataset Dataset cuaca untuk prediksi suhu dan kondisi lingkungan. - Energy Efficiency Dataset Dataset untuk prediksi konsumsi energi pada bangunan. - Battery Dataset Data performa baterai untuk analisis degradasi. 5. Dataset untuk Keamanan dan Deteksi Anomali - NSL-KDD Dataset deteksi serangan siber dalam jaringan komputer. - Power Grid Stability Dataset Data stabilitas jaringan listrik. - Water Treatment Plant Dataset Dataset pemantauan kualitas air untuk deteksi anomali. 2.2.a Pra-Pemrosesan Data Ini adalah tahap awal yang bertujuan untuk membersihkan dan menyusun data agar lebih siap digunakan dalam analisis atau pembelajaran mesin. Proses ini mencakup beberapa langkah utama, yaitu: 1. Data Integration (Integrasi Data) : Menggabungkan data dari berbagai sumber agar menjadi satu kesatuan yang dapat dianalisis. 2. Schema Integration: Menyamakan struktur data dari berbagai sumber. 3. Entity Identification Problem: Menyamakan identitas entitas yang sama tetapi memiliki format berbeda di beberapa sumber data. 4. Detecting and Resolving Data Values Concepts: Mengidentifikasi dan menyelesaikan perbedaan nilai data yang memiliki makna serupa. graph TD; A[Data Preprocessing] --> B[Data Integration] B --> C[Schema integration] B --> D[Entity identification problem] B --> E[Detecting and resolving data values concepts] A --> F[Data Reduction or Dimension Reduction] F --> G[Data cube aggregation] F --> H[Attribute subset selection] F --> I[Numerosity reduction] F --> J[Dimensionality reduction] A --> K[Data Transformation] K --> L[Normalization] K --> M[Attribute selection] K --> N[Discretization] K --> O[Concept hierarchy generation] A --> P[Data Cleaning] P --> Q[Missing data] P --> R[Noisy data] 2.2.b Pra-Pemrosesan data Pra-Pemrosesan data (refrensi dari buku machine learning crash course for engineers ) juga bisa di lakukan dengan cara sederhana sebagai berikut yang akan di bahas lebih detail pada materi ini : 1. Data Wrangling 2. Feature Scaling 3. Data Types 4. Data Splitting a. Data Wrangling Data wrangling, juga dikenal sebagai data munging, adalah proses pra-pemrosesan data yang mencakup pengumpulan, pembersihan, transformasi, dan penggabungan data dari berbagai sumber menjadi bentuk yang dapat diolah. a.1. Explorasi Data import pandas as pd df = pd . read_csv ( \"data.csv\" ) print ( df . info ()) # Menampilkan tipe data dan nilai yang hilang print ( df . describe ()) # Statistik ringkasan data numerik print ( df . head ()) # Melihat 5 baris pertama a.2. Membersihkan data a.2.1. missing dengan imputasi X = [[ 1 , 5 ],[ 2 , 10 ], [ np . nan , 2 ],[ 7 , 35 ],[ 6 , np . nan ],[ 15 , 75 ],[ 0 , 0 ]] print ( X ) [[1, 5], [2, 10], [nan, 2], [7, 35], [6, nan], [15, 75], [0, 0]] import numpy as np from sklearn.impute import SimpleImputer # Menggunakan imputasi mean untuk mengganti nilai NaN imputer = SimpleImputer ( strategy = \"mean\" ) # Bisa diganti dengan \"median\" atau \"most_frequent\" X_imputed = imputer . fit_transform ( X ) print ( X_imputed ) [[ 1. 5. ] [ 2. 10. ] [ 5.16666667 2. ] [ 7. 35. ] [ 6. 21.16666667] [15. 75. ] [ 0. 0. ]] a.2.2.Menghapus duplikasi data df . drop_duplicates ( inplace = True ) a.2.3.Mengoreksi format data df [ \"tanggal\" ] = pd . to_datetime ( df [ \"tanggal\" ]) # Mengubah ke format datetime df [ \"harga\" ] = df [ \"harga\" ] . astype ( float ) # Mengubah tipe data a.3. Menggabungkan Data df_merged = pd . merge ( df1 , df2 , on = \"ID\" ) b. Feature Scaling Saat bekerja dengan berbagai fitur atau variabel, hampir dapat dipastikan bahwa kita memiliki data pada berbagai fitur dalam rentang yang berbeda. Kasus , satu set data memiliki dua fitur: suhu dan kelembapan. Nilai untuk suhu berkisar antara 25 hingga 0,35 \u25e6 C, dan nilai untuk kelembapan berkisar antara 70 hingga 90%. Rentangnya berbeda untuk kedua fitur tersebut, sehingga keduanya tidak dapat dibandingkan. Perbedaan rentang ini juga tidak sesuai untuk digunakan oleh algoritme ML. Oleh karena itu, semacam transformasi dilakukan pada nilai atribut sehingga semua fitur berada dalam rentang yang sebanding dan dapat digunakan . Transformasi yang dilakukan pada data untuk tujuan ini adalah penskalaan fitur . Ada berbagai metode untuk penskalaan fitur, tiga di antaranya adalah standarisasi, normalisasi, dan penambahan data (data agumentation) . Metode-metode ini dijelaskan dalam bagian berikut. b.1. Standardisasi Standardisasi merupakan metode yang sangat populer untuk penskalaan fitur. Setelah menstandardisasi himpunan data, kita memperoleh nilai rata-rata nol dan simpangan baku satuan. Metode ini digunakan terutama dalam kasus-kasus di mana distribusi data mengikuti distribusi normal. Menstandardisasi data tidak membawa data ke rentang tertentu yang telah ditetapkan sebelumnya. Oleh karena itu, metode penskalaan fitur ini tidak terpengaruh oleh outlier . Setiap kali suatu fitur distandarisasi, setiap data terlebih dahulu dikurangi dari nilai rata-rata dan kemudian dibagi dengan simpangan baku. \\(X_{std} = \\frac{X - \\mu}{\\sigma}\\) b.2. Normalisasi Normalisasi adalah metode lain yang populer digunakan untuk penskalaan fitur. Normalisasi adalah teknik untuk mengubah nilai numerik yang berbeda menjadi rentang umum tanpa mendistorsi perbedaan antara nilai-nilai tersebut. Ini menskalakan semua nilai fitur dalam rentang [0,1] atau [.\u22121,1]. Karena ada nilai rentang tertentu yang menormalkan data, outlier yang ada dalam kumpulan data memengaruhi metode ini . Normalisasi bermanfaat ketika distribusi data kumpulan data tidak diketahui. \\(X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}}\\) Kondisi Gunakan Normalisasi Gunakan Standarisasi Data tidak berdistribusi normal \u2705 \u274c Data berdistribusi normal (Gaussian) \u274c \u2705 Ada outlier ekstrem \u274c \u2705 Algoritma berbasis jarak (KNN, NN, SVM-RBF) \u2705 \u274c Algoritma berbasis distribusi (Regresi, PCA, SVM-Linear) \u274c \u2705 Temperature = [ 25 , 21 , 30 , 35 , 34 , 33 , 32 , 33 , 24 , 23 ] Humidity = [ 78 , 75 , 88 , 72 , 83 , 79 , 76 , 88 , 85 , 77 ] print ( 'Temperature =' , Temperature , ' \\n Humidity =' , Humidity ) Temperature = [25, 21, 30, 35, 34, 33, 32, 33, 24, 23] Humidity = [78, 75, 88, 72, 83, 79, 76, 88, 85, 77] import numpy as np from sklearn.preprocessing import MinMaxScaler , StandardScaler # Data asli Temperature = np . array ([ 25 , 21 , 30 , 35 , 34 , 33 , 32 , 33 , 24 , 23 ]) . reshape ( - 1 , 1 ) Humidity = np . array ([ 78 , 75 , 88 , 72 , 83 , 79 , 76 , 88 , 85 , 77 ]) . reshape ( - 1 , 1 ) # Normalisasi (Min-Max Scaling) minmax_scaler = MinMaxScaler () Temperature_norm = minmax_scaler . fit_transform ( Temperature ) Humidity_norm = minmax_scaler . fit_transform ( Humidity ) # Standarisasi (Z-score Scaling) std_scaler = StandardScaler () Temperature_std = std_scaler . fit_transform ( Temperature ) Humidity_std = std_scaler . fit_transform ( Humidity ) # Menampilkan hasil print ( \"Hasil Normalisasi\" ) print ( \"Temperature:\" , Temperature_norm . flatten ()) print ( \"Humidity:\" , Humidity_norm . flatten ()) print ( \" \\n Hasil Standarisasi (Z-score Scaling)\" ) print ( \"Temperature:\" , Temperature_std . flatten ()) print ( \"Humidity:\" , Humidity_std . flatten ()) Hasil Normalisasi Temperature: [0.28571429 0. 0.64285714 1. 0.92857143 0.85714286 0.78571429 0.85714286 0.21428571 0.14285714] Humidity: [0.375 0.1875 1. 0. 0.6875 0.4375 0.25 1. 0.8125 0.3125] Hasil Standarisasi (Z-score Scaling) Temperature: [-0.80977633 -1.61955266 0.20244408 1.2146645 1.01222041 0.80977633 0.60733225 0.80977633 -1.01222041 -1.2146645 ] Humidity: [-0.39622642 -0.96226415 1.49056604 -1.52830189 0.54716981 -0.20754717 -0.77358491 1.49056604 0.9245283 -0.58490566] c. Data Type Algoritme pembelajaran mesin menangani berbagai jenis data. Oleh karena itu, berbagai jenis data memerlukan pendekatan yang berbeda terhadap algoritme ML. Jenis data dapat dikategorikan sebagai (a.1.) berurutan dan tidak berurutan dan (a.2) stasioner dan tidak stasioner. c.1. Berurutan dan tidak berurutan Tipe data sekuensial adalah tipe data yang memiliki urutan tertentu, seperti daftar, string, tupel, urutan byte, array byte, dan objek rentang. Elemen dalam tipe data tersebut dapat diakses melalui indeksnya, yang menunjukkan posisi mereka dalam urutan dan dimulai dari 0. Gambar (a) Tipe data non-sekuensial tidak memiliki urutan, seperti kamus ( dictoneries ) dan set. Tidak ada urutan yang dipertahankan di antara elemen dalam tipe data non-sekuensial. Gambar (b) \\ c.2. Stasioner dan non-stasioner Kumpulan data stasioner diketahui memiliki sifat statistik yang konstan, seperti rata-rata, varians, dll., seiring waktu. Data dalam kumpulan data tersebut mudah diramalkan karena sifatnya tidak berubah seiring waktu. Kumpulan data non-stasioner yaitu kumpulan data yang sifat statistiknya berubah seiring waktu . Jenis data ini tidak dapat diramalkan atau dimodelkan karena sifatnya yang bervariasi. Data non-stasioner memiliki tren, siklus, atau musiman di dalamnya. Gambar 3.5 menunjukkan contoh data stasioner dan non-stasioner. d. Data Spliting Pemisahan data berfungsi untuk melatih model ML dengan tepat. Pemisahan yang tepat mencegah model dari overfitting, memastikan penilaian yang tepat, dan meningkatkan kinerja model. Cara pemisahan data yang paling umum adalah - dengan membagi dataset menjadi dua subset\u2014satu dataset pelatihan dan satu lagi dataset pengujian. - namun, konvensi pemisahan menjadi tiga subset juga sudah umum. Dalam kasus ini, dataset dibagi menjadi dataset pelatihan, validasi, dan pengujian. 3. Tugas Mahasiswa Silahkan lakukan pra-pemrosesan Data pada dataset Anda yang sudah dicari, jika belum ada gunakan dataset yang sudah disediakan!","title":"Pertemuan 3: Pengumpulan, Pemrosesan dan Visualisasi pada Dataset"},{"location":"Materi2/#pertemuan-3-pengumpulan-pemrosesan-dan-visualisasi-pada-dataset","text":"[TKE1686] MK Machine Learning (3-SKS) oleh Gramandha Wega Intyanto","title":"Pertemuan 3: Pengumpulan, Pemrosesan dan Visualisasi pada Dataset"},{"location":"Materi2/#1-alur-kerja-pada-machine-learning","text":"graph LR; A[\"Tahap 1\\nPengumpulan Dataset\"] --> B[\"Tahap 2\\nPemrosesan Dataset\"]; B --> C[\"Tahap 3\\nPembelajaran Model\"]; C --> D[\"Tahap 4\\nEvaluasi Model\"]; D -.->|Perbaikan pemrosesan Dataset| B; D -.->|Revisi Model| C; D -.->|Perbaikan Dataset| A; Pengumpulan Dataset: Langkah awal ML adalah mengumpulkan kumpulan data. Hal-hal yang harus dipertimbangakan dalam pengumpulan data yaitu metode pengambilan data, data akan digunakan untuk apa? Preproses Data: Data yang kami kumpulkan sering kali tidak terorganisir dan tidak dapat langsung digunakan untuk melatih model. Sebelum melanjutkan ke langkah berikutnya, data perlu diproses terlebih dahulu. Kumpulan data mungkin berisi data yang hilang atau berisik, perlu di perbaiki atau di filter ; Data yang berbeda dapat berada dalam rentang yang berbeda, perlu dilakukan standarisasi atau normalisasi karena bisa jadi menjadi masalah bagi model; Perlu menemukan dan memilih data yang lebih berkontribusi untuk menemukan variabel target; Terakhir, kumpulan data harus dibagi menjadi dua set yaitu pelatihan dan pengujian (bisa juga dilakukan pembagian menjadi 3 yaitu pelatihan, validasi, pengujian ) | yang sering digunakan 80% set pelatihan, 20% set pengujian / 70% set pelatihan 20% set validasi 10% set pengujian. Melatih Model: Berdasarkan permasalahan, jenis model yang dibutuhkan harus dipilih terlebih dahulu. Saat memilih model, informasi yang tersedia pada kumpulan data harus dipertimbangkan. Evaluasi Model: Setelah model dibangun dan dilatih, penting untuk memahami seberapa baik model telah dilatih, seberapa baik kinerjanya, dan apakah model akan berguna untuk eksperimen. Kumpulan data set pengujiandigunakan untuk menguji model , dan berbagai metrik evaluasi digunakan untuk mengevaluasi dan memahami model.","title":"1. Alur Kerja pada Machine Learning"},{"location":"Materi2/#2-dataset","text":"Performa model sangat bergantung pada persiapan kumpulan data (dataset). Biasanya Kumpulan data paling baik disusun dalam bentuk tabel, di mana setiap baris sesuai dengan satu entri data dan setiap kolom mewakili variabel tertentu yang terkait dengan semua data yang tersedia dalam kumpulan data tersebut. Ukuran kumpulan data yang berkisar dari terabyte hingga megabyte dapat disebut sebagai kumpulan data besar, sedangkan kumpulan data dalam kisaran kilobyte adalah kumpulan data kecil.","title":"2. Dataset"},{"location":"Materi2/#21-pengumpulan-data","text":"a. Data yang diambil langsung dari secara langsung dengan beberapa komponen sensor-sensor. - Kamera dan sensor optik untuk pengambilan data citra. - Senosr IMU untuk pengambilan data percepatan, rotasi dan orientasi. - Bisa juga dataset kumpulan beberapa sensor seperti data loging - dsb. b. Data yang diambil langsung dari beberapa platform data public - (roboflow, keaggle, coco, pascal voc, dsb) Beberapa Rekomendasi Dataset untuk Machine Learning dalam Teknik Elektro 1. Dataset untuk Pengolahan Citra (Computer Vision) - MNIST Dataset angka tulisan tangan (0-9) untuk klasifikasi. - CIFAR-10 & CIFAR-100 Dataset gambar untuk klasifikasi objek. - COCO (Common Objects in Context) Dataset objek dunia nyata untuk deteksi dan segmentasi. - Open Electrical Substation Images Dataset gambar infrastruktur listrik. 2. Dataset untuk Pemrosesan Sinyal dan Sistem Tenaga - EEG Brainwave Dataset Data sinyal EEG untuk analisis otak. - ECG Heartbeat Categorization Dataset Sinyal jantung ECG untuk klasifikasi kondisi kesehatan. - Electricity Load Forecasting Dataset konsumsi energi listrik untuk prediksi. - Fault Detection in Power Systems Dataset deteksi gangguan pada jaringan listrik. 3. Dataset untuk Kendali dan Robotika - Self-Driving Car Dataset (Udacity) Dataset untuk pengendalian kendaraan otonom. - MIT Push Dataset Dataset untuk robotik dan manipulasi objek. - CMU Motion Capture Dataset Data pergerakan tubuh manusia untuk kontrol robot. 4. Dataset untuk Prediksi dan Analisis Data - Weather Dataset Dataset cuaca untuk prediksi suhu dan kondisi lingkungan. - Energy Efficiency Dataset Dataset untuk prediksi konsumsi energi pada bangunan. - Battery Dataset Data performa baterai untuk analisis degradasi. 5. Dataset untuk Keamanan dan Deteksi Anomali - NSL-KDD Dataset deteksi serangan siber dalam jaringan komputer. - Power Grid Stability Dataset Data stabilitas jaringan listrik. - Water Treatment Plant Dataset Dataset pemantauan kualitas air untuk deteksi anomali.","title":"2.1. Pengumpulan Data"},{"location":"Materi2/#22a-pra-pemrosesan-data","text":"Ini adalah tahap awal yang bertujuan untuk membersihkan dan menyusun data agar lebih siap digunakan dalam analisis atau pembelajaran mesin. Proses ini mencakup beberapa langkah utama, yaitu: 1. Data Integration (Integrasi Data) : Menggabungkan data dari berbagai sumber agar menjadi satu kesatuan yang dapat dianalisis. 2. Schema Integration: Menyamakan struktur data dari berbagai sumber. 3. Entity Identification Problem: Menyamakan identitas entitas yang sama tetapi memiliki format berbeda di beberapa sumber data. 4. Detecting and Resolving Data Values Concepts: Mengidentifikasi dan menyelesaikan perbedaan nilai data yang memiliki makna serupa. graph TD; A[Data Preprocessing] --> B[Data Integration] B --> C[Schema integration] B --> D[Entity identification problem] B --> E[Detecting and resolving data values concepts] A --> F[Data Reduction or Dimension Reduction] F --> G[Data cube aggregation] F --> H[Attribute subset selection] F --> I[Numerosity reduction] F --> J[Dimensionality reduction] A --> K[Data Transformation] K --> L[Normalization] K --> M[Attribute selection] K --> N[Discretization] K --> O[Concept hierarchy generation] A --> P[Data Cleaning] P --> Q[Missing data] P --> R[Noisy data]","title":"2.2.a Pra-Pemrosesan Data"},{"location":"Materi2/#22b-pra-pemrosesan-data","text":"Pra-Pemrosesan data (refrensi dari buku machine learning crash course for engineers ) juga bisa di lakukan dengan cara sederhana sebagai berikut yang akan di bahas lebih detail pada materi ini : 1. Data Wrangling 2. Feature Scaling 3. Data Types 4. Data Splitting","title":"2.2.b Pra-Pemrosesan data"},{"location":"Materi2/#a-data-wrangling","text":"Data wrangling, juga dikenal sebagai data munging, adalah proses pra-pemrosesan data yang mencakup pengumpulan, pembersihan, transformasi, dan penggabungan data dari berbagai sumber menjadi bentuk yang dapat diolah.","title":"a. Data Wrangling"},{"location":"Materi2/#a1-explorasi-data","text":"import pandas as pd df = pd . read_csv ( \"data.csv\" ) print ( df . info ()) # Menampilkan tipe data dan nilai yang hilang print ( df . describe ()) # Statistik ringkasan data numerik print ( df . head ()) # Melihat 5 baris pertama","title":"a.1. Explorasi Data"},{"location":"Materi2/#a2-membersihkan-data","text":"a.2.1. missing dengan imputasi X = [[ 1 , 5 ],[ 2 , 10 ], [ np . nan , 2 ],[ 7 , 35 ],[ 6 , np . nan ],[ 15 , 75 ],[ 0 , 0 ]] print ( X ) [[1, 5], [2, 10], [nan, 2], [7, 35], [6, nan], [15, 75], [0, 0]] import numpy as np from sklearn.impute import SimpleImputer # Menggunakan imputasi mean untuk mengganti nilai NaN imputer = SimpleImputer ( strategy = \"mean\" ) # Bisa diganti dengan \"median\" atau \"most_frequent\" X_imputed = imputer . fit_transform ( X ) print ( X_imputed ) [[ 1. 5. ] [ 2. 10. ] [ 5.16666667 2. ] [ 7. 35. ] [ 6. 21.16666667] [15. 75. ] [ 0. 0. ]] a.2.2.Menghapus duplikasi data df . drop_duplicates ( inplace = True ) a.2.3.Mengoreksi format data df [ \"tanggal\" ] = pd . to_datetime ( df [ \"tanggal\" ]) # Mengubah ke format datetime df [ \"harga\" ] = df [ \"harga\" ] . astype ( float ) # Mengubah tipe data","title":"a.2. Membersihkan data"},{"location":"Materi2/#a3-menggabungkan-data","text":"df_merged = pd . merge ( df1 , df2 , on = \"ID\" )","title":"a.3. Menggabungkan Data"},{"location":"Materi2/#b-feature-scaling","text":"Saat bekerja dengan berbagai fitur atau variabel, hampir dapat dipastikan bahwa kita memiliki data pada berbagai fitur dalam rentang yang berbeda. Kasus , satu set data memiliki dua fitur: suhu dan kelembapan. Nilai untuk suhu berkisar antara 25 hingga 0,35 \u25e6 C, dan nilai untuk kelembapan berkisar antara 70 hingga 90%. Rentangnya berbeda untuk kedua fitur tersebut, sehingga keduanya tidak dapat dibandingkan. Perbedaan rentang ini juga tidak sesuai untuk digunakan oleh algoritme ML. Oleh karena itu, semacam transformasi dilakukan pada nilai atribut sehingga semua fitur berada dalam rentang yang sebanding dan dapat digunakan . Transformasi yang dilakukan pada data untuk tujuan ini adalah penskalaan fitur . Ada berbagai metode untuk penskalaan fitur, tiga di antaranya adalah standarisasi, normalisasi, dan penambahan data (data agumentation) . Metode-metode ini dijelaskan dalam bagian berikut.","title":"b. Feature Scaling"},{"location":"Materi2/#b1-standardisasi","text":"Standardisasi merupakan metode yang sangat populer untuk penskalaan fitur. Setelah menstandardisasi himpunan data, kita memperoleh nilai rata-rata nol dan simpangan baku satuan. Metode ini digunakan terutama dalam kasus-kasus di mana distribusi data mengikuti distribusi normal. Menstandardisasi data tidak membawa data ke rentang tertentu yang telah ditetapkan sebelumnya. Oleh karena itu, metode penskalaan fitur ini tidak terpengaruh oleh outlier . Setiap kali suatu fitur distandarisasi, setiap data terlebih dahulu dikurangi dari nilai rata-rata dan kemudian dibagi dengan simpangan baku. \\(X_{std} = \\frac{X - \\mu}{\\sigma}\\)","title":"b.1. Standardisasi"},{"location":"Materi2/#b2-normalisasi","text":"Normalisasi adalah metode lain yang populer digunakan untuk penskalaan fitur. Normalisasi adalah teknik untuk mengubah nilai numerik yang berbeda menjadi rentang umum tanpa mendistorsi perbedaan antara nilai-nilai tersebut. Ini menskalakan semua nilai fitur dalam rentang [0,1] atau [.\u22121,1]. Karena ada nilai rentang tertentu yang menormalkan data, outlier yang ada dalam kumpulan data memengaruhi metode ini . Normalisasi bermanfaat ketika distribusi data kumpulan data tidak diketahui. \\(X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}}\\) Kondisi Gunakan Normalisasi Gunakan Standarisasi Data tidak berdistribusi normal \u2705 \u274c Data berdistribusi normal (Gaussian) \u274c \u2705 Ada outlier ekstrem \u274c \u2705 Algoritma berbasis jarak (KNN, NN, SVM-RBF) \u2705 \u274c Algoritma berbasis distribusi (Regresi, PCA, SVM-Linear) \u274c \u2705 Temperature = [ 25 , 21 , 30 , 35 , 34 , 33 , 32 , 33 , 24 , 23 ] Humidity = [ 78 , 75 , 88 , 72 , 83 , 79 , 76 , 88 , 85 , 77 ] print ( 'Temperature =' , Temperature , ' \\n Humidity =' , Humidity ) Temperature = [25, 21, 30, 35, 34, 33, 32, 33, 24, 23] Humidity = [78, 75, 88, 72, 83, 79, 76, 88, 85, 77] import numpy as np from sklearn.preprocessing import MinMaxScaler , StandardScaler # Data asli Temperature = np . array ([ 25 , 21 , 30 , 35 , 34 , 33 , 32 , 33 , 24 , 23 ]) . reshape ( - 1 , 1 ) Humidity = np . array ([ 78 , 75 , 88 , 72 , 83 , 79 , 76 , 88 , 85 , 77 ]) . reshape ( - 1 , 1 ) # Normalisasi (Min-Max Scaling) minmax_scaler = MinMaxScaler () Temperature_norm = minmax_scaler . fit_transform ( Temperature ) Humidity_norm = minmax_scaler . fit_transform ( Humidity ) # Standarisasi (Z-score Scaling) std_scaler = StandardScaler () Temperature_std = std_scaler . fit_transform ( Temperature ) Humidity_std = std_scaler . fit_transform ( Humidity ) # Menampilkan hasil print ( \"Hasil Normalisasi\" ) print ( \"Temperature:\" , Temperature_norm . flatten ()) print ( \"Humidity:\" , Humidity_norm . flatten ()) print ( \" \\n Hasil Standarisasi (Z-score Scaling)\" ) print ( \"Temperature:\" , Temperature_std . flatten ()) print ( \"Humidity:\" , Humidity_std . flatten ()) Hasil Normalisasi Temperature: [0.28571429 0. 0.64285714 1. 0.92857143 0.85714286 0.78571429 0.85714286 0.21428571 0.14285714] Humidity: [0.375 0.1875 1. 0. 0.6875 0.4375 0.25 1. 0.8125 0.3125] Hasil Standarisasi (Z-score Scaling) Temperature: [-0.80977633 -1.61955266 0.20244408 1.2146645 1.01222041 0.80977633 0.60733225 0.80977633 -1.01222041 -1.2146645 ] Humidity: [-0.39622642 -0.96226415 1.49056604 -1.52830189 0.54716981 -0.20754717 -0.77358491 1.49056604 0.9245283 -0.58490566]","title":"b.2. Normalisasi"},{"location":"Materi2/#c-data-type","text":"Algoritme pembelajaran mesin menangani berbagai jenis data. Oleh karena itu, berbagai jenis data memerlukan pendekatan yang berbeda terhadap algoritme ML. Jenis data dapat dikategorikan sebagai (a.1.) berurutan dan tidak berurutan dan (a.2) stasioner dan tidak stasioner.","title":"c. Data Type"},{"location":"Materi2/#c1-berurutan-dan-tidak-berurutan","text":"Tipe data sekuensial adalah tipe data yang memiliki urutan tertentu, seperti daftar, string, tupel, urutan byte, array byte, dan objek rentang. Elemen dalam tipe data tersebut dapat diakses melalui indeksnya, yang menunjukkan posisi mereka dalam urutan dan dimulai dari 0. Gambar (a) Tipe data non-sekuensial tidak memiliki urutan, seperti kamus ( dictoneries ) dan set. Tidak ada urutan yang dipertahankan di antara elemen dalam tipe data non-sekuensial. Gambar (b) \\","title":"c.1. Berurutan dan tidak berurutan"},{"location":"Materi2/#c2-stasioner-dan-non-stasioner","text":"Kumpulan data stasioner diketahui memiliki sifat statistik yang konstan, seperti rata-rata, varians, dll., seiring waktu. Data dalam kumpulan data tersebut mudah diramalkan karena sifatnya tidak berubah seiring waktu. Kumpulan data non-stasioner yaitu kumpulan data yang sifat statistiknya berubah seiring waktu . Jenis data ini tidak dapat diramalkan atau dimodelkan karena sifatnya yang bervariasi. Data non-stasioner memiliki tren, siklus, atau musiman di dalamnya. Gambar 3.5 menunjukkan contoh data stasioner dan non-stasioner.","title":"c.2. Stasioner dan non-stasioner"},{"location":"Materi2/#d-data-spliting","text":"Pemisahan data berfungsi untuk melatih model ML dengan tepat. Pemisahan yang tepat mencegah model dari overfitting, memastikan penilaian yang tepat, dan meningkatkan kinerja model. Cara pemisahan data yang paling umum adalah - dengan membagi dataset menjadi dua subset\u2014satu dataset pelatihan dan satu lagi dataset pengujian. - namun, konvensi pemisahan menjadi tiga subset juga sudah umum. Dalam kasus ini, dataset dibagi menjadi dataset pelatihan, validasi, dan pengujian.","title":"d. Data Spliting"},{"location":"Materi2/#3-tugas-mahasiswa","text":"Silahkan lakukan pra-pemrosesan Data pada dataset Anda yang sudah dicari, jika belum ada gunakan dataset yang sudah disediakan!","title":"3. Tugas Mahasiswa"},{"location":"Materi3/","text":"Pertemuan 4-5: Klaster (K-Means, Hirarki Klaster, DBSCAN) [TKE1686] MK Machine Learning (3-SKS) oleh: Gramandha Wega Intyanto, S.ST., M.T. Algoritma Klaster yang akan di pelajari K-Means Hirarki Klaster DBSCAN K-Means Algoritma K-means adalah salah satu algoritma clustering yang bersifat iteratif yang mencoba untuk mempartisi dataset menjadi subkelompok non-overlapping berbeda yang ditentukan oleh K (cluster) di mana setiap titik data hanya dimiliki oleh satu kelompok. K-Means mencoba membuat titik data intra-cluster semirip mungkin sambil dengan titik data yang lain pada satu cluster. K-Means menetapkan poin data ke cluster sedemikian rupa sehingga jumlah jarak kuadrat antara titik data dan pusat massa cluster (rata-rata aritmatika dari semua titik data yang termasuk dalam cluster itu) minimal. Semakin sedikit variasi yang kita miliki dalam cluster, semakin homogen (serupa) titik data dalam cluster yang sama. Langkah-Langkah K-Means Memilih jumlah cluster awal (K) yang ingin dibuat Memilih titik secara random sebanyak K buah, di mana titik ini akan menjadi pusat (centroid) dari masing-masing kelompok (clusters). Dari dataset yang kita miliki, buat dataset yang terdekat dengan titik centroid sebagai bagian dari cluster tersebut. Sehingga secara total akan terbentuk clusters sebanyak K buah. Lakukan kalkulasi, dan tempatkan pusat centroid yang baru untuk setiap cluster-nya. Langkah ini dilakukan untuk menemukan centroid yang paling tepat untuk maisng-masing klaster. Dari dataset yang kita miliki ambil titik centroid terdekat, sehingga dataset tadi menjadi bagian dari cluster tersebut. Jika masih ada data yang berubah kelompok (pindah cluster), kembali ke langkah Jika tidak, maka cluster yang terbentuk sudah baik. Penetuan ngukuran jarak bisa menggunakan beberapa metode, seperti contohnya: Ecluidean Distance : Jarak dihitung sebagai panjang garis lurus antara dua titik dalam ruang Euclidean. Ini adalah bentuk jarak yang paling umum digunakan dalam analisis data dan pembelajaran mesin. Formula Euclidean Distance untuk dua titik \\((x_1, y_1)\\) dan \\((x_2, y_2)\\) : \\(d_{\\text{Euclidean}}(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\\) Manhattan Distance : Jarak dihitung sebagai jumlah selisih absolut antara koordinat titik data dan centroid. Formula Manhattan Distance untuk dua titik \\((x_1, y_1)\\) dan \\((x_2, y_2)\\) : \\(d_{\\text{Manhattan}}(x, c) = \\sum_{i=1}^{n} |x_i - y_i|\\) atau menggunakan metode lain Minkowski, Cosine, Mahalanobis, Chebyshev Distance Contoh Program K-Means # Mengimpor library import numpy as np import matplotlib.pyplot as plt import pandas as pd # Mengimpor dataset dataset = pd . read_csv ( 'datasets/Customer.csv' ) X = dataset . iloc [:, [ 3 , 4 ]] . values # Menggunakan metode elbow untuk menentukan angka cluster yang tepat from sklearn.cluster import KMeans wcss = [] for i in range ( 1 , 11 ): kmeans = KMeans ( n_clusters = i , init = 'k-means++' , random_state = 42 ) kmeans . fit ( X ) wcss . append ( kmeans . inertia_ ) plt . plot ( range ( 1 , 11 ), wcss ) plt . title ( 'Elbow Method' ) plt . xlabel ( 'Cluster Number' ) plt . ylabel ( 'WCSS' ) plt . show () # Menjalankan K-Means Clustering ke dataset kmeans = KMeans ( n_clusters = 5 , init = 'k-means++' , random_state = 42 ) y_kmeans = kmeans . fit_predict ( X ) # Visualisasi hasil clusters plt . scatter ( X [ y_kmeans == 0 , 0 ], X [ y_kmeans == 0 , 1 ], s = 100 , c = 'blue' , label = 'Cluster 1' ) plt . scatter ( X [ y_kmeans == 1 , 0 ], X [ y_kmeans == 1 , 1 ], s = 100 , c = 'red' , label = 'Cluster 2' ) plt . scatter ( X [ y_kmeans == 2 , 0 ], X [ y_kmeans == 2 , 1 ], s = 100 , c = 'magenta' , label = 'Cluster 3' ) plt . scatter ( X [ y_kmeans == 3 , 0 ], X [ y_kmeans == 3 , 1 ], s = 100 , c = 'cyan' , label = 'Cluster 4' ) plt . scatter ( X [ y_kmeans == 4 , 0 ], X [ y_kmeans == 4 , 1 ], s = 100 , c = 'green' , label = 'Cluster 5' ) plt . scatter ( kmeans . cluster_centers_ [:, 0 ], kmeans . cluster_centers_ [:, 1 ], s = 300 , c = 'yellow' , label = 'Centroids' ) plt . title ( 'Consumers Cluster' ) plt . xlabel ( 'Yearly Salary' ) plt . ylabel ( 'Yearly expense rating (1-100)' ) plt . legend () plt . show () Hirarki Klaster (Hirarical Cluster) Pengelompokan hierarki adalah Teknik clustering dengan memisahkan data ke dalam kelompok berdasarkan beberapa ukuran kesamaan, menemukan cara untuk mengukur bagaimana mereka sama dan berbeda, dan selanjutnya mempersempit data. Contoh kasus: Pertama, terdapat empat mobil yang dapat masukkan ke dalam dua kelompok jenis mobil: sedan dan SUV. Selanjutnya, HC akan menggabungkan sedan dan SUV. Untuk langkah terakhir, yaitu mengelompokkan semuanya ke dalam satu cluster dan selesai ketika kita hanya memiliki satu cluster. Tipe dari Hirarki klaster Divisive: Pengelompokan divisif dikenal sebagai pendekatan top-down, yaitu mengambil cluster besar dan mulai membaginya menjadi dua, tiga, empat, atau lebih cluster. Agglomerative: Pengelompokan aglomeratif dikenal sebagai pendekatan bottom-up, yaitu pengelompokan dimulai dari cluster kecil menuju satu cluster besar. Langkah-langkah metode hierarchical clustering dengan agglomerative: Buat setiap data poin dalam dataset menjadi sebuah cluster, sehi8ngga untuk N data kita memiliki N cluster. Misalnya jika jumlah row data adalah 500 maka akan terdapat 500 cluster. Cari dua poin/2 cluster yang saling berdekatan untuk digabung menjadi satu cluster sehingga jumlah cluster menjadi lebih kecil. Cari 2 cluster lagi yang berdekatan dengan yang lain (termasuk dengan kluster yang baru saja dibuat di langkah 2 jika memang cluster tersebut memiliki jarak terdekat dengan kluster lain), dan jadikan dua cluster terdekat ini menjadi 1 kluster. Dengan demikian, sekarang kita memiliki N-2 kluster. Langkah ketiga akan diulang terus hingga mendapatkan satu buah cluster besar. Menggunakan Dendogram Sama seperti pada contoh hierarchical clustering sebelumnya, terdapat enam titik dalam satu diagram. Grafik yang atas adalah grafik awal dan yang bawah adalah grafik dendogram. Sama seperti ilustrasi pada hierarchical clustering, langkah pertama adalah menentukan dua titik terdekat kemudian menerjemahkannya ke dalam diagram dendogram. Mencari lagi dua cluster yang berdekatan untuk digabungkan menjadi satu cluster lagi. Tinggi diagram Dendogram berbeda-beda sesuai dengan hasil penghitungan Euclidean Distancenya. Proses yang ketiga diulang lagi dengan mencari dua cluster yang terdekat. Jika dua cluster yang digabungkan sebelumnya adalah cluster antara dua titik maka cara menerjemahkan dalam dendogram. Mengulang proses dengan menggabungkan cluster yang sudah ada, Pada gambar disamping tampak bahwa dua cluster terakhir merupakan gabungan dari cluster (dua titik yang menjadi satu cluster) pada proses awal. Proses akan berhenti setelah semua cluster telah tergabung menjadi satu cluster besar. Untuk menentukan berapa jumlah cluster yang paling sesuai pada dats set yang diujikan dapat dianalisa melalui dendogram. Yaitu dengan menentukan garis grafik dendogram yang paling panjang yang tidak terkena potongan atau bisa juga dengan menentukan nilai threshold. Contoh program hirarki klaster # Mengimpor library import numpy as np import matplotlib.pyplot as plt import pandas as pd import scipy.cluster.hierarchy as sch from sklearn.cluster import AgglomerativeClustering # Mengimpor dataset try : dataset = pd . read_csv ( 'datasets/Customer.csv' ) print ( \"Dataset berhasil dimuat!\" ) except FileNotFoundError : print ( \"Error: File 'Customer.csv' tidak ditemukan. Pastikan file berada di direktori yang benar.\" ) exit () # Pastikan dataset memiliki cukup kolom if dataset . shape [ 1 ] < 5 : print ( \"Error: Dataset tidak memiliki kolom yang cukup untuk clustering.\" ) exit () # Mengambil fitur yang akan digunakan X = dataset . iloc [:, [ 3 , 4 ]] . values # Menampilkan dendrogram untuk menentukan jumlah cluster plt . figure ( figsize = ( 10 , 5 )) dendrogram = sch . dendrogram ( sch . linkage ( X , method = 'ward' )) plt . title ( 'Dendrogram' ) plt . xlabel ( 'Consumer' ) plt . ylabel ( 'Euclidean Distance' ) plt . show () # Menjalankan Hierarchical Clustering dengan parameter yang diperbaiki hc = AgglomerativeClustering ( n_clusters = 5 , metric = 'euclidean' , linkage = 'ward' ) y_hc = hc . fit_predict ( X ) # Visualisasi hasil clustering plt . figure ( figsize = ( 8 , 6 )) colors = [ 'red' , 'blue' , 'green' , 'cyan' , 'magenta' ] labels = [ 'Cluster 1' , 'Cluster 2' , 'Cluster 3' , 'Cluster 4' , 'Cluster 5' ] for i in range ( 5 ): plt . scatter ( X [ y_hc == i , 0 ], X [ y_hc == i , 1 ], s = 100 , c = colors [ i ], label = labels [ i ]) plt . title ( 'Consumers Cluster' ) plt . xlabel ( 'Yearly Salary' ) plt . ylabel ( 'Yearly Expense Rating (1-100)' ) plt . legend () plt . show () Dataset berhasil dimuat! DBSCAN (Density-Based Spatial Clustering of Applications) Density-Based Spatial Clustering of Applications with Noise (DBSCAN) adalah algoritma dasar untuk pengelompokan berbasis density. Algoritma ini dapat menemukan cluster dengan berbagai bentuk dan ukuran dari sejumlah besar data, yang mengandung noise dan outlier. ketika data cluster berbentuk arbiter atau ingin mendeteksi cluster out lier, maka DBSCAN merupaka Teknik cluster yang sesuai Algoritma DBSCAN menggunakan dua parameter yaitu: minPts : Jumlah minimum titik (ambang batas) yang dikelompokkan bersama agar suatu wilayah dianggap density. eps (\u03b5) : Ukuran jarak yang akan digunakan untuk menemukan titik-titik di sekitar titik mana pun. Kedua parameter ini dapat diterapkan dengan baik dengan menggunakan dua konsep yaitu Density Reachability dan Density Connectivity Konsep Reachability pada konsep ini, untuk menentukan kepadatan dialukan dengan menetapkan suatu titik yang dapat dijangkau dari yang lain jika terletak dalam jarak tertentu (eps) darinya. Connectivity , konsep ini melakukan pendekatan chaining berbasis transitivitas untuk menentukan apakah titik terletak di cluster tertentu. Misalnya, titik p dan q dapat dihubungkan jika \\(p->r->s->t->q\\) , di mana \\(x->y\\) berarti \\(x\\) berada di sekitar (neighborhood) \\(y\\) . Langkah-langkah algoritma DBSCAN Algoritma dimulai dengan mengambil titik dalam kumpulan data secara random (sampai semua titik telah dikunjungi). Jika setidaknya ada titik 'minPoint' dalam radius ke titik tersebut, maka dapat dianggap semua titik ini sebagai bagian dari cluster yang sama. Cluster kemudian diperluas dengan mengulangi perhitungan lingkungan secara rekursif untuk setiap titik tetangga. Terdapat tiga jenis titik setelah pengelompokan DBSCAN selesai: Core adalah titik yang memiliki setidaknya m titik dalam jarak n dari dirinya sendiri. Border adalah titik yang memiliki setidaknya satu titik Inti pada jarak n. Noise adalah titik yang bukan Core atau Border. Dan ia memiliki kurang dari m titik dalam jarak n dari dirinya sendiri. Contoh program DBSCAN #Penerapan DBSCAN pada cluster spherical data. import numpy as np from sklearn.cluster import DBSCAN from sklearn import metrics from sklearn.datasets import make_blobs from sklearn.preprocessing import StandardScaler # Generate sample data centers = [[ 1 , 1 ], [ - 1 , - 1 ], [ 1 , - 1 ]] X , labels_true = make_blobs ( n_samples = 500 , centers = centers , cluster_std = 0.4 , random_state = 0 ) X = StandardScaler () . fit_transform ( X ) # Menghitung DBSCAN db = DBSCAN ( eps = 0.3 , min_samples = 10 ) . fit ( X ) core_samples_mask = np . zeros_like ( db . labels_ , dtype = bool ) core_samples_mask [ db . core_sample_indices_ ] = True labels = db . labels_ # Number of clusters in labels, ignoring noise if present. n_clusters_ = len ( set ( labels )) - ( 1 if - 1 in labels else 0 ) n_noise_ = list ( labels ) . count ( - 1 ) print ( 'Estimated number of clusters: %d ' % n_clusters_ ) print ( 'Estimated number of noise points: %d ' % n_noise_ ) # print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels)) # print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels)) # print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels)) # print(\"Adjusted Rand Index: %0.3f\" # % metrics.adjusted_rand_score(labels_true, labels)) # print(\"Adjusted Mutual Information: %0.3f\" # % metrics.adjusted_mutual_info_score(labels_true, labels)) # print(\"Silhouette Coefficient: %0.3f\" # % metrics.silhouette_score(X, labels)) # Plot result import matplotlib.pyplot as plt # Black removed and is used for noise instead. unique_labels = set ( labels ) colors = [ plt . cm . Spectral ( each ) for each in np . linspace ( 0 , 1 , len ( unique_labels ))] for k , col in zip ( unique_labels , colors ): if k == - 1 : # Black used for noise. col = [ 0 , 0 , 0 , 1 ] class_member_mask = ( labels == k ) xy = X [ class_member_mask & core_samples_mask ] plt . plot ( xy [:, 0 ], xy [:, 1 ], 'o' , markerfacecolor = tuple ( col ), markeredgecolor = 'k' , markersize = 14 ) xy = X [ class_member_mask & ~ core_samples_mask ] plt . plot ( xy [:, 0 ], xy [:, 1 ], 'o' , markerfacecolor = tuple ( col ), markeredgecolor = 'k' , markersize = 6 ) plt . title ( 'Estimated number of clusters: %d ' % n_clusters_ ) plt . show () Estimated number of clusters: 3 Estimated number of noise points: 20 Tugas Materi Klaster Cari dataset yang perlu untuk di klaster atau dari dataset yang Anda punya, silahkan lakukan klaster dari featur yang Anda tentukan, kemudian klaster dengan K-mean Klaster, Hirarki Klaster, dan DBscan.","title":"Pertemuan 4-5: Klaster (K-Means, Hirarki Klaster, DBSCAN)"},{"location":"Materi3/#pertemuan-4-5-klaster-k-means-hirarki-klaster-dbscan","text":"[TKE1686] MK Machine Learning (3-SKS) oleh: Gramandha Wega Intyanto, S.ST., M.T.","title":"Pertemuan 4-5: Klaster (K-Means, Hirarki Klaster, DBSCAN)"},{"location":"Materi3/#algoritma-klaster-yang-akan-di-pelajari","text":"K-Means Hirarki Klaster DBSCAN","title":"Algoritma Klaster yang akan di pelajari"},{"location":"Materi3/#k-means","text":"Algoritma K-means adalah salah satu algoritma clustering yang bersifat iteratif yang mencoba untuk mempartisi dataset menjadi subkelompok non-overlapping berbeda yang ditentukan oleh K (cluster) di mana setiap titik data hanya dimiliki oleh satu kelompok. K-Means mencoba membuat titik data intra-cluster semirip mungkin sambil dengan titik data yang lain pada satu cluster. K-Means menetapkan poin data ke cluster sedemikian rupa sehingga jumlah jarak kuadrat antara titik data dan pusat massa cluster (rata-rata aritmatika dari semua titik data yang termasuk dalam cluster itu) minimal. Semakin sedikit variasi yang kita miliki dalam cluster, semakin homogen (serupa) titik data dalam cluster yang sama.","title":"K-Means"},{"location":"Materi3/#langkah-langkah-k-means","text":"Memilih jumlah cluster awal (K) yang ingin dibuat Memilih titik secara random sebanyak K buah, di mana titik ini akan menjadi pusat (centroid) dari masing-masing kelompok (clusters). Dari dataset yang kita miliki, buat dataset yang terdekat dengan titik centroid sebagai bagian dari cluster tersebut. Sehingga secara total akan terbentuk clusters sebanyak K buah. Lakukan kalkulasi, dan tempatkan pusat centroid yang baru untuk setiap cluster-nya. Langkah ini dilakukan untuk menemukan centroid yang paling tepat untuk maisng-masing klaster. Dari dataset yang kita miliki ambil titik centroid terdekat, sehingga dataset tadi menjadi bagian dari cluster tersebut. Jika masih ada data yang berubah kelompok (pindah cluster), kembali ke langkah Jika tidak, maka cluster yang terbentuk sudah baik.","title":"Langkah-Langkah K-Means"},{"location":"Materi3/#penetuan-ngukuran-jarak-bisa-menggunakan-beberapa-metode-seperti-contohnya","text":"Ecluidean Distance : Jarak dihitung sebagai panjang garis lurus antara dua titik dalam ruang Euclidean. Ini adalah bentuk jarak yang paling umum digunakan dalam analisis data dan pembelajaran mesin. Formula Euclidean Distance untuk dua titik \\((x_1, y_1)\\) dan \\((x_2, y_2)\\) : \\(d_{\\text{Euclidean}}(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\\) Manhattan Distance : Jarak dihitung sebagai jumlah selisih absolut antara koordinat titik data dan centroid. Formula Manhattan Distance untuk dua titik \\((x_1, y_1)\\) dan \\((x_2, y_2)\\) : \\(d_{\\text{Manhattan}}(x, c) = \\sum_{i=1}^{n} |x_i - y_i|\\) atau menggunakan metode lain Minkowski, Cosine, Mahalanobis, Chebyshev Distance","title":"Penetuan ngukuran jarak bisa menggunakan beberapa metode, seperti contohnya:"},{"location":"Materi3/#contoh-program-k-means","text":"# Mengimpor library import numpy as np import matplotlib.pyplot as plt import pandas as pd # Mengimpor dataset dataset = pd . read_csv ( 'datasets/Customer.csv' ) X = dataset . iloc [:, [ 3 , 4 ]] . values # Menggunakan metode elbow untuk menentukan angka cluster yang tepat from sklearn.cluster import KMeans wcss = [] for i in range ( 1 , 11 ): kmeans = KMeans ( n_clusters = i , init = 'k-means++' , random_state = 42 ) kmeans . fit ( X ) wcss . append ( kmeans . inertia_ ) plt . plot ( range ( 1 , 11 ), wcss ) plt . title ( 'Elbow Method' ) plt . xlabel ( 'Cluster Number' ) plt . ylabel ( 'WCSS' ) plt . show () # Menjalankan K-Means Clustering ke dataset kmeans = KMeans ( n_clusters = 5 , init = 'k-means++' , random_state = 42 ) y_kmeans = kmeans . fit_predict ( X ) # Visualisasi hasil clusters plt . scatter ( X [ y_kmeans == 0 , 0 ], X [ y_kmeans == 0 , 1 ], s = 100 , c = 'blue' , label = 'Cluster 1' ) plt . scatter ( X [ y_kmeans == 1 , 0 ], X [ y_kmeans == 1 , 1 ], s = 100 , c = 'red' , label = 'Cluster 2' ) plt . scatter ( X [ y_kmeans == 2 , 0 ], X [ y_kmeans == 2 , 1 ], s = 100 , c = 'magenta' , label = 'Cluster 3' ) plt . scatter ( X [ y_kmeans == 3 , 0 ], X [ y_kmeans == 3 , 1 ], s = 100 , c = 'cyan' , label = 'Cluster 4' ) plt . scatter ( X [ y_kmeans == 4 , 0 ], X [ y_kmeans == 4 , 1 ], s = 100 , c = 'green' , label = 'Cluster 5' ) plt . scatter ( kmeans . cluster_centers_ [:, 0 ], kmeans . cluster_centers_ [:, 1 ], s = 300 , c = 'yellow' , label = 'Centroids' ) plt . title ( 'Consumers Cluster' ) plt . xlabel ( 'Yearly Salary' ) plt . ylabel ( 'Yearly expense rating (1-100)' ) plt . legend () plt . show ()","title":"Contoh Program K-Means"},{"location":"Materi3/#hirarki-klaster-hirarical-cluster","text":"Pengelompokan hierarki adalah Teknik clustering dengan memisahkan data ke dalam kelompok berdasarkan beberapa ukuran kesamaan, menemukan cara untuk mengukur bagaimana mereka sama dan berbeda, dan selanjutnya mempersempit data.","title":"Hirarki Klaster (Hirarical Cluster)"},{"location":"Materi3/#contoh-kasus","text":"Pertama, terdapat empat mobil yang dapat masukkan ke dalam dua kelompok jenis mobil: sedan dan SUV. Selanjutnya, HC akan menggabungkan sedan dan SUV. Untuk langkah terakhir, yaitu mengelompokkan semuanya ke dalam satu cluster dan selesai ketika kita hanya memiliki satu cluster.","title":"Contoh kasus:"},{"location":"Materi3/#tipe-dari-hirarki-klaster","text":"Divisive: Pengelompokan divisif dikenal sebagai pendekatan top-down, yaitu mengambil cluster besar dan mulai membaginya menjadi dua, tiga, empat, atau lebih cluster. Agglomerative: Pengelompokan aglomeratif dikenal sebagai pendekatan bottom-up, yaitu pengelompokan dimulai dari cluster kecil menuju satu cluster besar.","title":"Tipe dari Hirarki klaster"},{"location":"Materi3/#langkah-langkah-metode-hierarchical-clustering-dengan-agglomerative","text":"Buat setiap data poin dalam dataset menjadi sebuah cluster, sehi8ngga untuk N data kita memiliki N cluster. Misalnya jika jumlah row data adalah 500 maka akan terdapat 500 cluster. Cari dua poin/2 cluster yang saling berdekatan untuk digabung menjadi satu cluster sehingga jumlah cluster menjadi lebih kecil. Cari 2 cluster lagi yang berdekatan dengan yang lain (termasuk dengan kluster yang baru saja dibuat di langkah 2 jika memang cluster tersebut memiliki jarak terdekat dengan kluster lain), dan jadikan dua cluster terdekat ini menjadi 1 kluster. Dengan demikian, sekarang kita memiliki N-2 kluster. Langkah ketiga akan diulang terus hingga mendapatkan satu buah cluster besar.","title":"Langkah-langkah metode hierarchical clustering dengan agglomerative:"},{"location":"Materi3/#menggunakan-dendogram","text":"Sama seperti pada contoh hierarchical clustering sebelumnya, terdapat enam titik dalam satu diagram. Grafik yang atas adalah grafik awal dan yang bawah adalah grafik dendogram. Sama seperti ilustrasi pada hierarchical clustering, langkah pertama adalah menentukan dua titik terdekat kemudian menerjemahkannya ke dalam diagram dendogram. Mencari lagi dua cluster yang berdekatan untuk digabungkan menjadi satu cluster lagi. Tinggi diagram Dendogram berbeda-beda sesuai dengan hasil penghitungan Euclidean Distancenya. Proses yang ketiga diulang lagi dengan mencari dua cluster yang terdekat. Jika dua cluster yang digabungkan sebelumnya adalah cluster antara dua titik maka cara menerjemahkan dalam dendogram. Mengulang proses dengan menggabungkan cluster yang sudah ada, Pada gambar disamping tampak bahwa dua cluster terakhir merupakan gabungan dari cluster (dua titik yang menjadi satu cluster) pada proses awal. Proses akan berhenti setelah semua cluster telah tergabung menjadi satu cluster besar. Untuk menentukan berapa jumlah cluster yang paling sesuai pada dats set yang diujikan dapat dianalisa melalui dendogram. Yaitu dengan menentukan garis grafik dendogram yang paling panjang yang tidak terkena potongan atau bisa juga dengan menentukan nilai threshold.","title":"Menggunakan Dendogram"},{"location":"Materi3/#contoh-program-hirarki-klaster","text":"# Mengimpor library import numpy as np import matplotlib.pyplot as plt import pandas as pd import scipy.cluster.hierarchy as sch from sklearn.cluster import AgglomerativeClustering # Mengimpor dataset try : dataset = pd . read_csv ( 'datasets/Customer.csv' ) print ( \"Dataset berhasil dimuat!\" ) except FileNotFoundError : print ( \"Error: File 'Customer.csv' tidak ditemukan. Pastikan file berada di direktori yang benar.\" ) exit () # Pastikan dataset memiliki cukup kolom if dataset . shape [ 1 ] < 5 : print ( \"Error: Dataset tidak memiliki kolom yang cukup untuk clustering.\" ) exit () # Mengambil fitur yang akan digunakan X = dataset . iloc [:, [ 3 , 4 ]] . values # Menampilkan dendrogram untuk menentukan jumlah cluster plt . figure ( figsize = ( 10 , 5 )) dendrogram = sch . dendrogram ( sch . linkage ( X , method = 'ward' )) plt . title ( 'Dendrogram' ) plt . xlabel ( 'Consumer' ) plt . ylabel ( 'Euclidean Distance' ) plt . show () # Menjalankan Hierarchical Clustering dengan parameter yang diperbaiki hc = AgglomerativeClustering ( n_clusters = 5 , metric = 'euclidean' , linkage = 'ward' ) y_hc = hc . fit_predict ( X ) # Visualisasi hasil clustering plt . figure ( figsize = ( 8 , 6 )) colors = [ 'red' , 'blue' , 'green' , 'cyan' , 'magenta' ] labels = [ 'Cluster 1' , 'Cluster 2' , 'Cluster 3' , 'Cluster 4' , 'Cluster 5' ] for i in range ( 5 ): plt . scatter ( X [ y_hc == i , 0 ], X [ y_hc == i , 1 ], s = 100 , c = colors [ i ], label = labels [ i ]) plt . title ( 'Consumers Cluster' ) plt . xlabel ( 'Yearly Salary' ) plt . ylabel ( 'Yearly Expense Rating (1-100)' ) plt . legend () plt . show () Dataset berhasil dimuat!","title":"Contoh program hirarki klaster"},{"location":"Materi3/#dbscan-density-based-spatial-clustering-of-applications","text":"Density-Based Spatial Clustering of Applications with Noise (DBSCAN) adalah algoritma dasar untuk pengelompokan berbasis density. Algoritma ini dapat menemukan cluster dengan berbagai bentuk dan ukuran dari sejumlah besar data, yang mengandung noise dan outlier. ketika data cluster berbentuk arbiter atau ingin mendeteksi cluster out lier, maka DBSCAN merupaka Teknik cluster yang sesuai Algoritma DBSCAN menggunakan dua parameter yaitu: minPts : Jumlah minimum titik (ambang batas) yang dikelompokkan bersama agar suatu wilayah dianggap density. eps (\u03b5) : Ukuran jarak yang akan digunakan untuk menemukan titik-titik di sekitar titik mana pun. Kedua parameter ini dapat diterapkan dengan baik dengan menggunakan dua konsep yaitu Density Reachability dan Density Connectivity","title":"DBSCAN (Density-Based Spatial Clustering of Applications)"},{"location":"Materi3/#konsep","text":"Reachability pada konsep ini, untuk menentukan kepadatan dialukan dengan menetapkan suatu titik yang dapat dijangkau dari yang lain jika terletak dalam jarak tertentu (eps) darinya. Connectivity , konsep ini melakukan pendekatan chaining berbasis transitivitas untuk menentukan apakah titik terletak di cluster tertentu. Misalnya, titik p dan q dapat dihubungkan jika \\(p->r->s->t->q\\) , di mana \\(x->y\\) berarti \\(x\\) berada di sekitar (neighborhood) \\(y\\) .","title":"Konsep"},{"location":"Materi3/#langkah-langkah-algoritma-dbscan","text":"Algoritma dimulai dengan mengambil titik dalam kumpulan data secara random (sampai semua titik telah dikunjungi). Jika setidaknya ada titik 'minPoint' dalam radius ke titik tersebut, maka dapat dianggap semua titik ini sebagai bagian dari cluster yang sama. Cluster kemudian diperluas dengan mengulangi perhitungan lingkungan secara rekursif untuk setiap titik tetangga.","title":"Langkah-langkah algoritma DBSCAN"},{"location":"Materi3/#terdapat-tiga-jenis-titik-setelah-pengelompokan-dbscan-selesai","text":"Core adalah titik yang memiliki setidaknya m titik dalam jarak n dari dirinya sendiri. Border adalah titik yang memiliki setidaknya satu titik Inti pada jarak n. Noise adalah titik yang bukan Core atau Border. Dan ia memiliki kurang dari m titik dalam jarak n dari dirinya sendiri.","title":"Terdapat tiga jenis titik setelah pengelompokan DBSCAN selesai:"},{"location":"Materi3/#contoh-program-dbscan","text":"#Penerapan DBSCAN pada cluster spherical data. import numpy as np from sklearn.cluster import DBSCAN from sklearn import metrics from sklearn.datasets import make_blobs from sklearn.preprocessing import StandardScaler # Generate sample data centers = [[ 1 , 1 ], [ - 1 , - 1 ], [ 1 , - 1 ]] X , labels_true = make_blobs ( n_samples = 500 , centers = centers , cluster_std = 0.4 , random_state = 0 ) X = StandardScaler () . fit_transform ( X ) # Menghitung DBSCAN db = DBSCAN ( eps = 0.3 , min_samples = 10 ) . fit ( X ) core_samples_mask = np . zeros_like ( db . labels_ , dtype = bool ) core_samples_mask [ db . core_sample_indices_ ] = True labels = db . labels_ # Number of clusters in labels, ignoring noise if present. n_clusters_ = len ( set ( labels )) - ( 1 if - 1 in labels else 0 ) n_noise_ = list ( labels ) . count ( - 1 ) print ( 'Estimated number of clusters: %d ' % n_clusters_ ) print ( 'Estimated number of noise points: %d ' % n_noise_ ) # print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels)) # print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels)) # print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels)) # print(\"Adjusted Rand Index: %0.3f\" # % metrics.adjusted_rand_score(labels_true, labels)) # print(\"Adjusted Mutual Information: %0.3f\" # % metrics.adjusted_mutual_info_score(labels_true, labels)) # print(\"Silhouette Coefficient: %0.3f\" # % metrics.silhouette_score(X, labels)) # Plot result import matplotlib.pyplot as plt # Black removed and is used for noise instead. unique_labels = set ( labels ) colors = [ plt . cm . Spectral ( each ) for each in np . linspace ( 0 , 1 , len ( unique_labels ))] for k , col in zip ( unique_labels , colors ): if k == - 1 : # Black used for noise. col = [ 0 , 0 , 0 , 1 ] class_member_mask = ( labels == k ) xy = X [ class_member_mask & core_samples_mask ] plt . plot ( xy [:, 0 ], xy [:, 1 ], 'o' , markerfacecolor = tuple ( col ), markeredgecolor = 'k' , markersize = 14 ) xy = X [ class_member_mask & ~ core_samples_mask ] plt . plot ( xy [:, 0 ], xy [:, 1 ], 'o' , markerfacecolor = tuple ( col ), markeredgecolor = 'k' , markersize = 6 ) plt . title ( 'Estimated number of clusters: %d ' % n_clusters_ ) plt . show () Estimated number of clusters: 3 Estimated number of noise points: 20","title":"Contoh program DBSCAN"},{"location":"Materi3/#tugas-materi-klaster","text":"Cari dataset yang perlu untuk di klaster atau dari dataset yang Anda punya, silahkan lakukan klaster dari featur yang Anda tentukan, kemudian klaster dengan K-mean Klaster, Hirarki Klaster, dan DBscan.","title":"Tugas Materi Klaster"},{"location":"Materi4/","text":"Pertemuan 6: Klasifikasi [TKE1686] MK Machine Learning (3-SKS) oleh: Gramandha Wega Intyanto, S.ST., M.T. Algoritma Klasifikasi yang akan di pelajari - k-Nearest Neighbor (K-NN) - Decision Tree K-Nearest Neighbor (K-NN) Metode k-Nearest Neighbor Classifier menentukan kelas/label dari suatu data uji berdasarkan label dari data \u2013 data latih sekitarnya. Ilustrasi ditunjukkan pada Gambar berikut Algoritma untuk k-nearest neighbor sebagai beirkut : Menentukan nilai k Menghitung jarak antara data baru terhadap semua training data Mengidentifikasi k nearest neighbor Menentukan label/kelas data baru berdasarkan kelas k-nearest neighbor (dapat menggunakan voting) Karakter dari algoritma k-nearest neighbor dapat dituliskan sebagai beirkut : Cocok untuk data numerik Mudah dipahami dan diimplementasikan k-NN merupakan lazy learner (tidak membangun model secara eksplisit) Penentuan label/kelas data baru membutuhkan computational cost yang cukup tinggi Perlu menentukan nilai k yang sesuai: a. Jika k terlalu kecil, sensitif terhadap noise b. Jika k terlalu besar, nearest neigbor mungkin mencakup data dari kelas lain Video KNN Penejelasan K-NN Contoh Studi kasus Studi kasus di bawah ini menggunakan penetuan nilai k=4 untuk mencari x1 = 3 dan x2 = 7 termasuk dalam kelas mana ( GOOD atau BAD ) Contoh Program Decision Tree Decision Tree (Pohon Keputusan) merupakan salah satu algoritma pembelajaran mesin yang mengklasifikasi dengan mengambil suatu keputusan antara benar atau tidaknya suatu aturan. Kelebihan dari algoritma Decision Tree adalah input yang digunakan boleh berupa tipe data apapun (String, Integer, Float, Boolean, dll), Video Decision Tree Contoh studi kasus Bagian paling puncak dari pohon keputusan (Hari Libur?) disebut dengan root node. Bagian selanjutnya dari pohon keputusan adalah branch, pada gambar di atas digambarkan oleh node (Jam > 17.00?). Terakhir leaves merupakan node yang berisi kelas dari permasalahan yang ingin diklasifikasikan. Salah satu cara membangun Decision Tree adalah dengan menggunakan perhitungan Gini Impurity. Gini Impurity untuk leaf dari Decision Tree dapat dihitung menggunakan persamaan berikut: $ \ud835\udc3a\ud835\udc56\ud835\udc5b\ud835\udc56 \ud835\udc3c\ud835\udc5a\ud835\udc5d\ud835\udc62\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc66 = 1 \u2212 (\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc4f\ud835\udc4e\ud835\udc4f\ud835\udc56\ud835\udc59\ud835\udc56\ud835\udc61\ud835\udc4e\ud835\udc60(\ud835\udc4c\ud835\udc4e))^2 \u2212 (\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc4f\ud835\udc4e\ud835\udc4f\ud835\udc56\ud835\udc59\ud835\udc56\ud835\udc61\ud835\udc4e\ud835\udc60(\ud835\udc47\ud835\udc56\ud835\udc51\ud835\udc4e\ud835\udc58))^2 $ Untuk menentukan root, perlu dicari Gini Impurity terkecil dari \u201cKebutuhan Primer\u201d, \u201cSudah Gajian\u201d, dan \u201cHarga Barang\u201d terhadap \u201cBeli\u201d. Gini Impurity \u201cKebutuhan Primer\u201d terhadap \u201cBeli\u201d dapat diilustrasikan pada Gambar berikut Total Gini Impurity dapat dihitung dengan weighted average dari Gini Impurity kedua leaves. Jumlah kemunculan \u201cYa\u201d untuk kebutuhan primer sebanyak 4 dan jumlah kemunculan \u201cTidak\u201d untuk kebutuhan primer sebanyak 3, maka weighted average Gini Impurity dapat dihitung dengan (terdapat di gambar): $ Gini Impurity leaf kiri * 4 / (4+3) + Gini Impurity leaf kanan * 3 / (4+3)$ Dengan cara yang sama, Gini Impurity \u201cSudah Gajian\u201d terhadap \u201cBeli\u201d dihitung dan diilustrasikan pada Gambar berikut: Karena \u201cHarga Barang\u201d memiliki variabel kontinu, Gini Impurity \u201cHarga Barang\u201d terhadap \u201cBeli\u201d dihitung dengan cara yang sedikit berbeda. Pertama, data harus diurutkan berdasarkan \u201cHarga Barang\u201d terkecil ke \u201cHarga Barang\u201d terbesar. Kemudian untuk setiap data yang berdekatan, hitung Gini Impurity untuk rata-rata \u201cHarga Barang\u201d kedua data yang berdekatan. Pada Gambar 20, dihitung Gini Impurity untuk \u201cHarga Barang\u201d > 9500. Dengan cara yang sama, hitung Gini Impurity untuk 15000, 26500, 36500, 44000, dan 66500. Setelah itu ambil Gini Impurity terkecil sebagai kandidat root. Jika ada lebih dari 1 yang sama, maka bebas menentukan yang mana yang ingin digunakan sebagai root. Setelah proses diatas, didapatkan Gini Impurity dari \u201cKebutuhan Primer\u201d, \u201cSudah Gajian\u201d, dan \u201cHarga Barang\u201d terhadap \u201cBeli\u201d secara berurutan 0.4, 0.22, dan 0.34. Karena Gini Impurity \u201cSudah Gajian\u201d terhadap \u201dBeli\u201d merupakan yang terkecil (0.22), maka \u201cSudah Gajian\u201d dijadikan root dari Decision Tree. Maka Decision Tree sementara menjadi seperti pada Gambar berikut Karena Gini Impurity leaf kanan adalah 0, maka tidak perlu di-split lagi. Oleh karena itu hanya leaf kiri yang perlu di-split. Leaf kiri dapat di-split dengan mencari Gini Impurity terkecil dari \u201cKebutuhan Primer\u201d atau \u201cHarga Barang\u201d. Cara yang dilakukan sama, namun data yang digunakan lebih sedikit karena telah tersaring oleh Decision root. Contoh untuk \u201cKebutuhan Primer\u201d dan \u201cHarga Barang\u201d tersaji pada Gambar berikut ini untuk data kontinu, perlu dihitung Gini Impurity dari data berdekatan. Karena Gini Impurity dari \u201cHarga Barang > 15000\u201d lebih kecil dibandingkan dengan Gini Impurity \u201cKebutuhan Primer\u201d, maka \u201cHarga Barang > 15000\u201d dipilih sebagai branch. Tidak diperlukan proses split lagi karena Gini Impurity dari seluruh leaves sudah mencapai 0. Sehingga Decision Tree yang dihasilkan adalah seperti pada Gambar Contoh Program Pada bahasa pemrograman Python, Decision Tree dapat dilakukan dengan menggunakan pustaka Scikit-Learn dengan menjalankan potongan kode Beberapa parameter dapat diubah nilainya, antara lain adalah parameter max_depth dan min_samples_split, dengan penjelasan pada tabel dan gambar berikut LKM 4 1. Soal Klasterisasi dengan k-means dan Hierarchical Clustering Deskripsi Soal Sebuah perusahaan ingin mengelompokkan pelanggan berdasarkan pola pembelian mereka. Data yang tersedia mencakup informasi penghasilan bulanan (dalam juta rupiah) dan jumlah transaksi dalam sebulan . Gunakan k-means dan Hierarchical Clustering untuk mengelompokkan pelanggan dan tentukan pola yang terbentuk. Data Pelanggan ID Pelanggan Penghasilan (Juta Rupiah) Jumlah Transaksi per Bulan P1 3.5 5 P2 7.0 20 P3 2.0 3 P4 5.5 10 P5 8.0 25 P6 1.5 2 P7 6.0 12 P8 9.5 30 P9 4.0 7 P10 3.0 4 Tugas a. K-means Clustering Jika kita sudah memiliki klaster awal dengan pusat klaster sebagai berikut: Klaster 1 : (2.5, 4) Klaster 2 : (7.5, 22) Gunakan metode k-means untuk mengelompokkan pelanggan ke dalam salah satu dari klaster tersebut berdasarkan jarak Euclidean. Tentukan klaster akhir setelah semua pelanggan dikategorikan. b. Hierarchical Clustering Gunakan metode Agglomerative Hierarchical Clustering dengan pendekatan Single Linkage atau Complete Linkage . Buat dendrogram untuk menunjukkan proses penggabungan klaster hingga terbentuk satu klaster besar. Tentukan jumlah klaster optimal berdasarkan dendrogram. 2. Soal Klasifikasi dengan k-NN dan Decision Tree Deskripsi Soal Sebuah bank ingin mengklasifikasikan calon nasabah ke dalam kategori \"Layak Kredit\" atau \"Tidak Layak Kredit\" berdasarkan beberapa parameter. Data yang tersedia mencakup penghasilan bulanan , jumlah pinjaman yang sedang berjalan , dan status pekerjaan . Gunakan k-NN dan Decision Tree untuk melakukan klasifikasi dan tentukan pola yang terbentuk. Data Nasabah ID Nasabah Penghasilan (Juta Rupiah) Pinjaman Aktif (Juta) Status Pekerjaan Status Kredit (Label) N1 5.0 10 Tetap Layak Kredit N2 2.5 5 Kontrak Tidak Layak Kredit N3 7.0 20 Tetap Layak Kredit N4 3.0 8 Kontrak Tidak Layak Kredit N5 6.0 15 Tetap Layak Kredit N6 4.0 12 Kontrak Tidak Layak Kredit N7 8.5 25 Tetap Layak Kredit N8 3.5 6 Kontrak Tidak Layak Kredit N9 6.5 18 Tetap Layak Kredit N10 2.0 4 Kontrak Tidak Layak Kredit Tugas a. K-NN Classification Gunakan metode k-Nearest Neighbors (k-NN) untuk mengklasifikasikan nasabah baru dengan fitur berikut: Penghasilan = 4.5 juta rupiah Pinjaman Aktif = 9 juta rupiah Status Pekerjaan = Kontrak Gunakan k = 3 dan hitung jarak Euclidean untuk menentukan kelas nasabah tersebut. b. Decision Tree Classification Gunakan metode Decision Tree untuk membangun model klasifikasi berdasarkan fitur yang tersedia. Gambarkan struktur decision tree yang terbentuk. Gunakan model untuk memprediksi status kredit nasabah dengan parameter berikut: Penghasilan = 6.0 juta rupiah Pinjaman Aktif = 14 juta rupiah Status Pekerjaan = Tetap","title":"Pertemuan 6: Klasifikasi"},{"location":"Materi4/#pertemuan-6-klasifikasi","text":"[TKE1686] MK Machine Learning (3-SKS) oleh: Gramandha Wega Intyanto, S.ST., M.T. Algoritma Klasifikasi yang akan di pelajari - k-Nearest Neighbor (K-NN) - Decision Tree","title":"Pertemuan 6: Klasifikasi"},{"location":"Materi4/#k-nearest-neighbor-k-nn","text":"Metode k-Nearest Neighbor Classifier menentukan kelas/label dari suatu data uji berdasarkan label dari data \u2013 data latih sekitarnya. Ilustrasi ditunjukkan pada Gambar berikut Algoritma untuk k-nearest neighbor sebagai beirkut : Menentukan nilai k Menghitung jarak antara data baru terhadap semua training data Mengidentifikasi k nearest neighbor Menentukan label/kelas data baru berdasarkan kelas k-nearest neighbor (dapat menggunakan voting) Karakter dari algoritma k-nearest neighbor dapat dituliskan sebagai beirkut : Cocok untuk data numerik Mudah dipahami dan diimplementasikan k-NN merupakan lazy learner (tidak membangun model secara eksplisit) Penentuan label/kelas data baru membutuhkan computational cost yang cukup tinggi Perlu menentukan nilai k yang sesuai: a. Jika k terlalu kecil, sensitif terhadap noise b. Jika k terlalu besar, nearest neigbor mungkin mencakup data dari kelas lain","title":"K-Nearest Neighbor (K-NN)"},{"location":"Materi4/#video-knn-penejelasan-k-nn","text":"","title":"Video KNN Penejelasan K-NN"},{"location":"Materi4/#contoh-studi-kasus","text":"Studi kasus di bawah ini menggunakan penetuan nilai k=4 untuk mencari x1 = 3 dan x2 = 7 termasuk dalam kelas mana ( GOOD atau BAD )","title":"Contoh Studi kasus"},{"location":"Materi4/#contoh-program","text":"","title":"Contoh Program"},{"location":"Materi4/#decision-tree","text":"Decision Tree (Pohon Keputusan) merupakan salah satu algoritma pembelajaran mesin yang mengklasifikasi dengan mengambil suatu keputusan antara benar atau tidaknya suatu aturan. Kelebihan dari algoritma Decision Tree adalah input yang digunakan boleh berupa tipe data apapun (String, Integer, Float, Boolean, dll), Video Decision Tree","title":"Decision Tree"},{"location":"Materi4/#contoh-studi-kasus_1","text":"Bagian paling puncak dari pohon keputusan (Hari Libur?) disebut dengan root node. Bagian selanjutnya dari pohon keputusan adalah branch, pada gambar di atas digambarkan oleh node (Jam > 17.00?). Terakhir leaves merupakan node yang berisi kelas dari permasalahan yang ingin diklasifikasikan. Salah satu cara membangun Decision Tree adalah dengan menggunakan perhitungan Gini Impurity. Gini Impurity untuk leaf dari Decision Tree dapat dihitung menggunakan persamaan berikut: $ \ud835\udc3a\ud835\udc56\ud835\udc5b\ud835\udc56 \ud835\udc3c\ud835\udc5a\ud835\udc5d\ud835\udc62\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc66 = 1 \u2212 (\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc4f\ud835\udc4e\ud835\udc4f\ud835\udc56\ud835\udc59\ud835\udc56\ud835\udc61\ud835\udc4e\ud835\udc60(\ud835\udc4c\ud835\udc4e))^2 \u2212 (\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc4f\ud835\udc4e\ud835\udc4f\ud835\udc56\ud835\udc59\ud835\udc56\ud835\udc61\ud835\udc4e\ud835\udc60(\ud835\udc47\ud835\udc56\ud835\udc51\ud835\udc4e\ud835\udc58))^2 $ Untuk menentukan root, perlu dicari Gini Impurity terkecil dari \u201cKebutuhan Primer\u201d, \u201cSudah Gajian\u201d, dan \u201cHarga Barang\u201d terhadap \u201cBeli\u201d. Gini Impurity \u201cKebutuhan Primer\u201d terhadap \u201cBeli\u201d dapat diilustrasikan pada Gambar berikut Total Gini Impurity dapat dihitung dengan weighted average dari Gini Impurity kedua leaves. Jumlah kemunculan \u201cYa\u201d untuk kebutuhan primer sebanyak 4 dan jumlah kemunculan \u201cTidak\u201d untuk kebutuhan primer sebanyak 3, maka weighted average Gini Impurity dapat dihitung dengan (terdapat di gambar): $ Gini Impurity leaf kiri * 4 / (4+3) + Gini Impurity leaf kanan * 3 / (4+3)$ Dengan cara yang sama, Gini Impurity \u201cSudah Gajian\u201d terhadap \u201cBeli\u201d dihitung dan diilustrasikan pada Gambar berikut: Karena \u201cHarga Barang\u201d memiliki variabel kontinu, Gini Impurity \u201cHarga Barang\u201d terhadap \u201cBeli\u201d dihitung dengan cara yang sedikit berbeda. Pertama, data harus diurutkan berdasarkan \u201cHarga Barang\u201d terkecil ke \u201cHarga Barang\u201d terbesar. Kemudian untuk setiap data yang berdekatan, hitung Gini Impurity untuk rata-rata \u201cHarga Barang\u201d kedua data yang berdekatan. Pada Gambar 20, dihitung Gini Impurity untuk \u201cHarga Barang\u201d > 9500. Dengan cara yang sama, hitung Gini Impurity untuk 15000, 26500, 36500, 44000, dan 66500. Setelah itu ambil Gini Impurity terkecil sebagai kandidat root. Jika ada lebih dari 1 yang sama, maka bebas menentukan yang mana yang ingin digunakan sebagai root. Setelah proses diatas, didapatkan Gini Impurity dari \u201cKebutuhan Primer\u201d, \u201cSudah Gajian\u201d, dan \u201cHarga Barang\u201d terhadap \u201cBeli\u201d secara berurutan 0.4, 0.22, dan 0.34. Karena Gini Impurity \u201cSudah Gajian\u201d terhadap \u201dBeli\u201d merupakan yang terkecil (0.22), maka \u201cSudah Gajian\u201d dijadikan root dari Decision Tree. Maka Decision Tree sementara menjadi seperti pada Gambar berikut Karena Gini Impurity leaf kanan adalah 0, maka tidak perlu di-split lagi. Oleh karena itu hanya leaf kiri yang perlu di-split. Leaf kiri dapat di-split dengan mencari Gini Impurity terkecil dari \u201cKebutuhan Primer\u201d atau \u201cHarga Barang\u201d. Cara yang dilakukan sama, namun data yang digunakan lebih sedikit karena telah tersaring oleh Decision root. Contoh untuk \u201cKebutuhan Primer\u201d dan \u201cHarga Barang\u201d tersaji pada Gambar berikut ini untuk data kontinu, perlu dihitung Gini Impurity dari data berdekatan. Karena Gini Impurity dari \u201cHarga Barang > 15000\u201d lebih kecil dibandingkan dengan Gini Impurity \u201cKebutuhan Primer\u201d, maka \u201cHarga Barang > 15000\u201d dipilih sebagai branch. Tidak diperlukan proses split lagi karena Gini Impurity dari seluruh leaves sudah mencapai 0. Sehingga Decision Tree yang dihasilkan adalah seperti pada Gambar","title":"Contoh studi kasus"},{"location":"Materi4/#contoh-program_1","text":"Pada bahasa pemrograman Python, Decision Tree dapat dilakukan dengan menggunakan pustaka Scikit-Learn dengan menjalankan potongan kode Beberapa parameter dapat diubah nilainya, antara lain adalah parameter max_depth dan min_samples_split, dengan penjelasan pada tabel dan gambar berikut","title":"Contoh Program"},{"location":"Materi4/#lkm-4","text":"","title":"LKM 4"},{"location":"Materi4/#1-soal-klasterisasi-dengan-k-means-dan-hierarchical-clustering","text":"","title":"1. Soal Klasterisasi dengan k-means dan Hierarchical Clustering"},{"location":"Materi4/#deskripsi-soal","text":"Sebuah perusahaan ingin mengelompokkan pelanggan berdasarkan pola pembelian mereka. Data yang tersedia mencakup informasi penghasilan bulanan (dalam juta rupiah) dan jumlah transaksi dalam sebulan . Gunakan k-means dan Hierarchical Clustering untuk mengelompokkan pelanggan dan tentukan pola yang terbentuk.","title":"Deskripsi Soal"},{"location":"Materi4/#data-pelanggan","text":"ID Pelanggan Penghasilan (Juta Rupiah) Jumlah Transaksi per Bulan P1 3.5 5 P2 7.0 20 P3 2.0 3 P4 5.5 10 P5 8.0 25 P6 1.5 2 P7 6.0 12 P8 9.5 30 P9 4.0 7 P10 3.0 4","title":"Data Pelanggan"},{"location":"Materi4/#tugas","text":"","title":"Tugas"},{"location":"Materi4/#a-k-means-clustering","text":"Jika kita sudah memiliki klaster awal dengan pusat klaster sebagai berikut: Klaster 1 : (2.5, 4) Klaster 2 : (7.5, 22) Gunakan metode k-means untuk mengelompokkan pelanggan ke dalam salah satu dari klaster tersebut berdasarkan jarak Euclidean. Tentukan klaster akhir setelah semua pelanggan dikategorikan.","title":"a. K-means Clustering"},{"location":"Materi4/#b-hierarchical-clustering","text":"Gunakan metode Agglomerative Hierarchical Clustering dengan pendekatan Single Linkage atau Complete Linkage . Buat dendrogram untuk menunjukkan proses penggabungan klaster hingga terbentuk satu klaster besar. Tentukan jumlah klaster optimal berdasarkan dendrogram.","title":"b. Hierarchical Clustering"},{"location":"Materi4/#2-soal-klasifikasi-dengan-k-nn-dan-decision-tree","text":"","title":"2. Soal Klasifikasi dengan k-NN dan Decision Tree"},{"location":"Materi4/#deskripsi-soal_1","text":"Sebuah bank ingin mengklasifikasikan calon nasabah ke dalam kategori \"Layak Kredit\" atau \"Tidak Layak Kredit\" berdasarkan beberapa parameter. Data yang tersedia mencakup penghasilan bulanan , jumlah pinjaman yang sedang berjalan , dan status pekerjaan . Gunakan k-NN dan Decision Tree untuk melakukan klasifikasi dan tentukan pola yang terbentuk.","title":"Deskripsi Soal"},{"location":"Materi4/#data-nasabah","text":"ID Nasabah Penghasilan (Juta Rupiah) Pinjaman Aktif (Juta) Status Pekerjaan Status Kredit (Label) N1 5.0 10 Tetap Layak Kredit N2 2.5 5 Kontrak Tidak Layak Kredit N3 7.0 20 Tetap Layak Kredit N4 3.0 8 Kontrak Tidak Layak Kredit N5 6.0 15 Tetap Layak Kredit N6 4.0 12 Kontrak Tidak Layak Kredit N7 8.5 25 Tetap Layak Kredit N8 3.5 6 Kontrak Tidak Layak Kredit N9 6.5 18 Tetap Layak Kredit N10 2.0 4 Kontrak Tidak Layak Kredit","title":"Data Nasabah"},{"location":"Materi4/#tugas_1","text":"","title":"Tugas"},{"location":"Materi4/#a-k-nn-classification","text":"Gunakan metode k-Nearest Neighbors (k-NN) untuk mengklasifikasikan nasabah baru dengan fitur berikut: Penghasilan = 4.5 juta rupiah Pinjaman Aktif = 9 juta rupiah Status Pekerjaan = Kontrak Gunakan k = 3 dan hitung jarak Euclidean untuk menentukan kelas nasabah tersebut.","title":"a. K-NN Classification"},{"location":"Materi4/#b-decision-tree-classification","text":"Gunakan metode Decision Tree untuk membangun model klasifikasi berdasarkan fitur yang tersedia. Gambarkan struktur decision tree yang terbentuk. Gunakan model untuk memprediksi status kredit nasabah dengan parameter berikut: Penghasilan = 6.0 juta rupiah Pinjaman Aktif = 14 juta rupiah Status Pekerjaan = Tetap","title":"b. Decision Tree Classification"}]}